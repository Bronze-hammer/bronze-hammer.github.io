<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ArrayList去重方式总结]]></title>
    <url>%2F2023%2F04%2F17%2FArrayList%E5%8E%BB%E9%87%8D%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[我在日常工作中常用Stream方式去重，满足了工作上业务的需求即可，并没有深入了解和尝试其他方式的去重操作，这对于个人的成长是很有局限性的，遂借此机会整理ArrayList的去重方法。 1.使用迭代器遍历去重 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// Java program to remove duplicates from ArrayListimport java.util.*;public class GFG &#123; // Function to remove duplicates from an ArrayList public static &lt;T&gt; ArrayList&lt;T&gt; removeDuplicates(ArrayList&lt;T&gt; list) &#123; // Create a new ArrayList ArrayList&lt;T&gt; newList = new ArrayList&lt;T&gt;(); // Traverse through the first list for (T element : list) &#123; // If this element is not present in newList // then add it if (!newList.contains(element)) &#123; newList.add(element); &#125; &#125; // return the new list return newList; &#125; // Driver code public static void main(String args[]) &#123; // Get the ArrayList with duplicate values ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;( Arrays .asList(1, 10, 1, 2, 2, 3, 3, 10, 3, 4, 5, 5)); // Print the Arraylist System.out.println("ArrayList with duplicates: " + list); // Remove duplicates ArrayList&lt;Integer&gt; newList = removeDuplicates(list); // Print the ArrayList with duplicates removed System.out.println("ArrayList with duplicates removed: " + newList); &#125;&#125; 输出： 12ArrayList with duplicates: [1, 10, 1, 2, 2, 3, 3, 10, 3, 4, 5, 5]ArrayList with duplicates removed: [1, 10, 2, 3, 4, 5] 2.使用LinkedHashSet 可以将ArrayList转换成不允许值重复的Set集合，因此LinkedHashSet是最好的选择，因为它不允许重复. HashSet也能实现同样的去重效果，但是HashSet与LinkedHashSet的不同之处在于，LinkedHashSet同时保留了插入顺序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// Java program to remove duplicates from ArrayListimport java.util.*;public class GFG &#123; // Function to remove duplicates from an ArrayList public static &lt;T&gt; ArrayList&lt;T&gt; removeDuplicates(ArrayList&lt;T&gt; list) &#123; // Create a new LinkedHashSet Set&lt;T&gt; set = new LinkedHashSet&lt;&gt;(); // Add the elements to set set.addAll(list); // Clear the list list.clear(); // add the elements of set // with no duplicates to the list list.addAll(set); // return the list return list; &#125; // Driver code public static void main(String args[]) &#123; // Get the ArrayList with duplicate values ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;( Arrays .asList(1, 10, 1, 2, 2, 3, 10, 3, 3, 4, 5, 5)); // Print the Arraylist System.out.println("ArrayList with duplicates: " + list); // Remove duplicates ArrayList&lt;Integer&gt; newList = removeDuplicates(list); // Print the ArrayList with duplicates removed System.out.println("ArrayList with duplicates removed: " + newList); &#125;&#125; 输出： 12ArrayList with duplicates: [1, 10, 1, 2, 2, 3, 10, 3, 3, 4, 5, 5]ArrayList with duplicates removed: [1, 10, 2, 3, 4, 5] 更简单的方法可以这样写 123Set&lt;String&gt; set = new LinkedHashSet&lt;&gt;(yourList);yourList.clear();yourList.addAll(set); 3.使用Java 8版本中的Stream.distinct()方法 distinct()方法根据equals()方法返回的结果返回一个没有重复元素的新Stream，可用于进一步处理。 12345678910111213141516171819202122232425262728// Java program to remove duplicates from ArrayListimport java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;// Program to remove duplicates from a List in Java 8class GFG&#123; public static void main(String[] args) &#123; // input list with duplicates List&lt;Integer&gt; list = new ArrayList&lt;&gt;( Arrays.asList(1, 10, 1, 2, 2, 3, 10, 3, 3, 4, 5, 5)); // Print the Arraylist System.out.println("ArrayList with duplicates: " + list); // Construct a new list from the set constucted from elements // of the original list List&lt;Integer&gt; newList = list.stream().distinct().collect(Collectors.toList()); // Print the ArrayList with duplicates removed System.out.println("ArrayList with duplicates removed: " + newList); &#125;&#125; 输出： 12ArrayList with duplicates: [1, 10, 1, 2, 2, 3, 10, 3, 3, 4, 5, 5]ArrayList with duplicates removed: [1, 10, 2, 3, 4, 5]]]></content>
      <categories>
        <category>代码人生</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[总结学习HashMap和HashTable]]></title>
    <url>%2F2023%2F04%2F14%2F%E6%80%BB%E7%BB%93%E5%AD%A6%E4%B9%A0HashMap%E5%92%8CHashTable%2F</url>
    <content type="text"><![CDATA[HashMap 和 Hashtable 都用于以键和值的形式存储数据。两者都使用散列技术来存储唯一密钥。但是HashMap和Hashtable 类之间也是有许多区别。 1.HashMap是不同步的，即非线程安全；Hashtable是同步的，即线程安全。 HashMap部分源码: 1234567891011121314151617181920212223242526272829// getpublic V get(Object key) &#123; HashMap.Node e; return (e = this.getNode(hash(key), key)) == null ? null : e.value;&#125;......// putpublic V put(K key, V value) &#123; return this.putVal(hash(key), key, value, false, true);&#125;......// removepublic V remove(Object key) &#123; HashMap.Node e; return (e = this.removeNode(hash(key), key, (Object)null, false, true)) == null ? null : e.value;&#125;...... HashTable部分源码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// getpublic synchronized V get(Object key) &#123; Hashtable.Entry&lt;?, ?&gt;[] tab = this.table; int hash = key.hashCode(); int index = (hash &amp; 2147483647) % tab.length; for(Hashtable.Entry e = tab[index]; e != null; e = e.next) &#123; if (e.hash == hash &amp;&amp; e.key.equals(key)) &#123; return e.value; &#125; &#125; return null;&#125;......// putpublic synchronized V put(K key, V value) &#123; if (value == null) &#123; throw new NullPointerException(); &#125; else &#123; Hashtable.Entry&lt;?, ?&gt;[] tab = this.table; int hash = key.hashCode(); int index = (hash &amp; 2147483647) % tab.length; for(Hashtable.Entry entry = tab[index]; entry != null; entry = entry.next) &#123; if (entry.hash == hash &amp;&amp; entry.key.equals(key)) &#123; V old = entry.value; entry.value = value; return old; &#125; &#125; this.addEntry(hash, key, value, index); return null; &#125;&#125;......// removepublic synchronized V remove(Object key) &#123; Hashtable.Entry&lt;?, ?&gt;[] tab = this.table; int hash = key.hashCode(); int index = (hash &amp; 2147483647) % tab.length; Hashtable.Entry&lt;K, V&gt; e = tab[index]; for(Hashtable.Entry prev = null; e != null; e = e.next) &#123; if (e.hash == hash &amp;&amp; e.key.equals(key)) &#123; if (prev != null) &#123; prev.next = e.next; &#125; else &#123; tab[index] = e.next; &#125; ++this.modCount; --this.count; V oldValue = e.value; e.value = null; return oldValue; &#125; prev = e; &#125; return null;&#125;...... 2.HashMap可以通过Collections.synchronizedMap(Map&lt;K, V&gt; m)实现同步；Hashtable不能实现非同步。 虽然HashMap不是线程安全的，但是我们可以通过Collections.synchronizedMap(Map&lt;K, V&gt; m)实现线程安全. 123456789101112131415161718192021public class App &#123; private static AtomicInteger atomicInteger = new AtomicInteger(); public static void main(String[] args) throws InterruptedException &#123; Map&lt;Integer, Integer&gt; m = new HashMap&lt;&gt;(); Map&lt;Integer, Integer&gt; map = Collections.synchronizedMap(m); //创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for(int i = 0 ; i &lt; 10000 ; i++) &#123; //调用execute()方法创建线程 threadPool.execute(() -&gt; map.put(atomicInteger.incrementAndGet(), (int)(Math.random()*100)) ); &#125; // 关闭线程池 threadPool.shutdown(); threadPool.awaitTermination(1000, TimeUnit.SECONDS); System.out.println(map.size()); &#125;&#125; 3.HashMap允许一个空键和多个空值；HashTable不允许任何空键和空值 从HashTable的源码可以看到，如果key或value是null，会抛出NullPointerException 12345678910111213141516171819202122232425262728293031323334353637383940/** * Maps the specified &lt;code&gt;key&lt;/code&gt; to the specified * &lt;code&gt;value&lt;/code&gt; in this hashtable. Neither the key nor the * value can be &lt;code&gt;null&lt;/code&gt;. &lt;p&gt; * * The value can be retrieved by calling the &lt;code&gt;get&lt;/code&gt; method * with a key that is equal to the original key. * * @param key the hashtable key * @param value the value * @return the previous value of the specified key in this hashtable, * or &lt;code&gt;null&lt;/code&gt; if it did not have one * @exception NullPointerException if the key or value is * &lt;code&gt;null&lt;/code&gt; * @see Object#equals(Object) * @see #get(Object) */public synchronized V put(K key, V value) &#123; // Make sure the value is not null if (value == null) &#123; throw new NullPointerException(); &#125; // Makes sure the key is not already in the hashtable. Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; for(; entry != null ; entry = entry.next) &#123; if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; V old = entry.value; entry.value = value; return old; &#125; &#125; addEntry(hash, key, value, index); return null;&#125; 4.HashMap是JDK 1.2中引入的新类. Hashtable是JDK 1.0中的类 5.HashMap比Hashtable更快. 6.HashMap由Iterator实现遍历. Hashtable由Enumerator和Iterator实现遍历. 7.HashMap中的迭代器是快速失败机制. Hashtable是安全失败机制. HashMap不是线程安全的，在遍历HashMap的内容时，如果有其他线程修改了HashMap的内容，那么将抛出ConcurrentModificationException。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// iteratorsabstract class HashIterator &#123; Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() &#123; expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125; public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; &#125;&#125; modCount用于记录修改次数，对HashMap的修改都将增加这个值，在迭代器初始化过程中会将modCount传递给expectedModCount。在迭代中就是根据modCount != expectedModCount判断Map是否已被其他线程修改。 Hashtable是fail-safe 安全失败机制 fail-safe:这种遍历基于容器的一个克隆。因此，对容器内容的修改不影响遍历。java.util.concurrent包下的容器都是安全失败的,可以在多线程下并发使用,并发修改。常见的的使用fail-safe方式遍历的容器有ConcerrentHashMap和CopyOnWriteArrayList等。 采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。 缺点：基于拷贝内容的优点是避免了ConcurrentModificationException，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。 8.HashMap继承AbstractMap类；Hashtable继承Dictionary类.]]></content>
      <categories>
        <category>代码人生</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>Hashtable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序猿三部曲之白银时代]]></title>
    <url>%2F2023%2F04%2F04%2F%E7%A8%8B%E5%BA%8F%E7%8C%BF%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E7%99%BD%E9%93%B6%E6%97%B6%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[之前在某篇公众号文章中，看到工资10K、15K、20K的Java程序员应该掌握的技术。大致对应着初、中、高级开发人员，所以我打算针对这三个阶段，写三篇文章，一边学习，一边总结。 曾经读过王小波的时代三部曲，分别是《青铜时代》、《白银时代》、《黄金时代》，遂借用来类比程序员的三个阶段。 1 HashMap和ConcurrentHashMap有什么区别？ HashMap是传统集合下的类，ConcurrentHashMap是并发集合下的类。除此之外，它们之间还有各种不同之处： HashMap本质上是非同步的，即HashMap不是线程安全的，而ConcurrentHashMap是线程安全的。 HashMap性能比较高，因为它是非同步的，任意数量的线程都可以同时访问它。而ConcurrentHashMap性能比较低，因为有时候线程需要在ConcurrentHashMap上等待请求。 当一个线程正在迭代HashMap时，如果有另外一个线程试图对这个HashMap的元素进行新增或者修改，我们将得到运行时异常 ConcurrentModificationException。然而，我们在迭代ConcurrentHashMap时执行任何修改都不会出现任何异常。 HashMap的key和value可以为null，ConcurrentHashMap不允许，否则会报运行时异常NullPointerException. HashMap 是在 JDK 1.2 中引入的，而 ConcurrentHashMap 是由 SUN Microsystem 在 JDK 1.5 中引入的 2 synchronized关键字 synchronized是Java中的关键字，是一种同步锁。它修饰的对象有以下几种： 修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象； 修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象； 修饰一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象。 修饰一个类，其作用的范围是synchronized后面括号括起来的部分，作用的对象是这个类的所有对象。 资料：Java中Synchronized的用法（简单介绍） 3 volatile关键字 123456789101112131415161718public class NoVisibility &#123; private static boolean ready; private static int number; private static class ReaderThread extends Thread &#123; @Override public void run() &#123; while(!ready) &#123; Thread.yield(); &#125; System.out.println(number); &#125; &#125; public static void main(String[] args) &#123; new ReaderThread().start(); number = 42; ready = true; &#125;&#125; NoVisibility可能会持续循环下去，因为读线程可能永远都看不到ready的值。甚至NoVisibility可能会输出0，因为读线程可能看到了写入ready的值，但却没有看到之后写入number的值，这种现象被称为“重排序”。只要在某个线程中无法检测到重排序情况（即使在其他线程中可以明显地看到该线程中的重排序），那么就无法确保线程中的操作将按照程序中指定的顺序来执行。当主线程首先写入number，然后在没有同步的情况下写入ready，那么读线程看到的顺序可能与写入的顺序完全相反。 3.1 volatile原理 Java语言提供了一种稍弱的同步机制，即volatile变量，用来确保将变量的更新操作通知到其他线程。当把变量声明为volatile类型后，编译器与运行时都会注意到这个变量是共享的，因此不会将该变量上的操作与其他内存操作一起重排序。volatile变量不会被缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。 在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一种比sychronized关键字更轻量级的同步机制。 当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到CPU缓存中。如果计算机有多个CPU，每个线程可能在不同的CPU上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。 而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。 3.2 当一个变量定义为volatile之后，将具备两种特性 保证此变量对所有的线程的可见性，这里的“可见性”，如本文开头所述，当一个线程修改了这个变量的值，volatile 保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。但普通变量做不到这点，普通变量的值在线程间传递均需要通过主内存（详见：Java内存模型）来完成。 禁止指令重排序优化。有volatile修饰的变量，赋值后多执行了一个“load addl $0x0, (%esp)”操作，这个操作相当于一个内存屏障（指令重排序时不能把后面的指令重排序到内存屏障之前的位置），只有一个CPU访问内存时，并不需要内存屏障；（什么是指令重排序：是指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理）。 3.3 volatile 性能： volatile 的读性能消耗与普通变量几乎相同，但是写操作稍慢，因为它需要在本地代码中插入许多内存屏障指令来保证处理器不发生乱序执行。 资料：Java中Volatile关键字详解 4 堆和栈的区别，堆中存放的是什么，栈中存放的是什么？ 4.1 堆空间 Java堆空间被Java运行时用来为对象和JRE类分配内存。每当我们创建一个对象时，它总是创建在Heap空间中。垃圾收集在堆内存上运行，以释放没有任何引用的对象所使用的内存。在堆空间中创建的任何对象都具有全局访问权，并且可以从应用程序的任何地方引用。 4.2 栈内存 Java Stack内存用于线程的执行。它们包含特定于方法的值，这些值存在时间很短，并引用从该方法引用的堆中其他对象。堆栈内存总是按照后进先出(LIFO)的顺序引用。每当调用一个方法时，就会在堆栈内存中为该方法创建一个新的块，用于保存本地原语值并引用该方法中的其他对象。方法一结束，该块就变为未使用的，并可用于下一个方法。与堆内存相比，堆栈内存大小要小得多。 4.3 栈和堆申请空间后系统的响应 栈：只要栈的剩余空间大于所申请的空间，系统将为程序提供内存，否则将报异常提示栈溢出。 堆：操作系统有一个记录空间内存地址的链表，当系统收到程序的申请时，会遍历链表，寻找第一个空间大于所申请空间的堆节点，然后将节点从内存空闲节点链表中删除，并将该节点的空间分配给程序。对于大多数操作系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的delete语句才能正确的释放本内存空间。另外，由于找到的对节点的大小不一定正好等于申请的大小，系统会自动地将多余的那部分重新放入到链表中。 4.4 Java 程序中的堆和栈内存 1234567891011121314151617package com.journaldev.test;public class Memory &#123; public static void main(String[] args) &#123; // Line 1 int i=1; // Line 2 Object obj = new Object(); // Line 3 Memory mem = new Memory(); // Line 4 mem.foo(obj); // Line 5 &#125; // Line 9 private void foo(Object param) &#123; // Line 6 String str = param.toString(); //// Line 7 System.out.println(str); &#125; // Line 8&#125; 下图显示了程序运行中堆空间和堆内存的引用，以及它们如何用于存储基元、对象和引用变量。 当我们运行该程序，系统会将所有运行时类加载到堆空间中。当在第一步执行main()方法时，Java Runtime会创建栈内存以供main()方法线程使用。 Line 2定义局部变量，它会被创建并存储到main()方法的栈内存中。 Line 3 new了一个Object对象，Object对象会在堆空间创建，而栈内存保存对象的引用obj，同理，Line 4也是一样的过程。 当我们在Line 5调用foo()方法时，栈内存会在顶部创建一个块以供foo()方法使用。 由于Java是按值传递的，因此在Line 6处的栈内存块中创建了对Object的新引用。 在Line 7创建一个字符串，它进入堆空间中的字符串池，并在foo()堆空间中为它创建一个引用。 foo()方法在Line 8终止，此时分配给foo()的堆栈内存块变为空闲。 在Line 9，main()方法终止，为main()方法创建的堆栈内存被销毁。此外，程序在此行结束，因此Java Runtime释放所有内存并结束程序的执行。 4.5 Java堆空间和栈内存的区别 基于上面的解释，我们可以很容易的得出以下Heap和Stack内存的区别。 栈内存仅能被一个线程执行，堆空间可以被程序中所有部分使用； 每当创建一个对象时，它总是存储在堆空间中，栈内存包含对它的引用。栈内存只包含本地原始变量和堆空间中对象的引用变量； 存储在堆中的对象是全局可访问的，而堆栈内存不能被其他线程访问； Memory management in stack is done in LIFO manner whereas it’s more complex in Heap memory because it’s used globally. Heap memory is divided into Young-Generation, Old-Generation etc, more details at Java Garbage Collection； 堆栈内存是短暂的，而堆内存从应用程序执行开始到结束都存在； 我们可以使用JVM的-Xms和-Xmx选项来定义堆内存的启动内存和最大内存。我们可以使用-Xss来定义栈内存大小； 当堆栈内存已满时，Java 运行时会抛出 java.lang.StackOverFlowError，而如果堆内存已满，则会抛出 java.lang.OutOfMemoryError: Java Heap Space 错误; 与堆内存相比，堆栈内存非常小。由于内存分配 (LIFO)的简单性，与堆内存相比，堆栈内存非常快。 资料：Java Heap Space vs Stack - Memory Allocation in Java 5 字符串池 顾名思义，java中的String Pool就是一个存储在Java Heap Memory中的Strings池。我们知道 String 是 java 中的一个特殊类，我们可以使用 new 运算符创建 String 对象，也可以在双引号中提供值。 5.1 Java中的字符串池 下面这张图很清楚的解释了String Pool在java堆空间中是如何维护的，以及当我们使用不同的方式创建String时会发生什么 当我们使用双引号创建一个字符串时，它首先在字符串池中寻找具有相同值的字符串，如果找到则返回引用，否则在池中创建一个新的字符串，然后返回引用。但是使用new运算符，我们强制String类在堆空间中创建一个新的String对象。我们可以使用 intern() 方法将其放入池中，或者从字符串池中引用另一个具有相同值的 String 对象。 123456789101112131415161718package com.journaldev.util;public class StringPool &#123; /** * Java String Pool example * @param args */ public static void main(String[] args) &#123; String s1 = "Cat"; String s2 = "Cat"; String s3 = new String("Cat"); System.out.println("s1 == s2 :"+(s1==s2)); System.out.println("s1 == s3 :"+(s1==s3)); &#125;&#125; 输出： 12s1 == s2 :trues1 == s3 :false 5.2 在字符串池中创建了多少字符串对象？ 有时候在java面试中，你会被问到一个关于String pool的问题。例如，在下面的语句中创建了多少个字符串对象？ 1String str = new String("Cat"); 首先在堆空间创建一个“Cat”对象，在栈内存创建str，并指向堆空间的“Cat”对象;然后检查堆空间中的字符串池中查看是否存在“Cat”对象，如果存在，则将new出来的“Cat”对象与字符串池中的“Cat”对象联系起来。若不存在，则在字符串池中创建“Cat”对象，并将堆中的“Cat”对象与之关联起来。 资料：What is Java String Pool? 未完，待续...]]></content>
      <categories>
        <category>代码人生</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>ConcurrentHashMap</tag>
        <tag>synchronized</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序猿三部曲之青铜时代]]></title>
    <url>%2F2023%2F03%2F14%2F%E7%A8%8B%E5%BA%8F%E7%8C%BF%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E9%9D%92%E9%93%9C%E6%97%B6%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[之前在某篇公众号文章中，看到工资10K、15K、20K的Java程序员应该掌握的技术。大致对应着初、中、高级开发人员，所以我打算针对这三个阶段，写三篇文章，一边学习，一边总结。 曾经读过王小波的时代三部曲，分别是《青铜时代》、《白银时代》、《黄金时代》，遂借用来类比程序员的三个阶段。 1 接口和抽象类的关系和区别 1.1 理解接口和抽象类 这个问题之前有了解过，也看过一些文章，但是好长时间不复习，也总是会忘记。因为工作中其实也不会很严谨的按照规则去使用接口和抽象类，总是按照controller、interface、mapper这样的通用格式，实现功能。借此机会，再次学习一下。 接口关心的是对象可以做什么，抽象类主要是描述对象是什么。定义一个狗狗的抽象类，这个类可以被藏獒继承、也可以被哈士奇、柯基继承，但是不能被人类继承。一个活动的接口，定义一个奔跑的方法，这个接口就不局限于藏獒和人类了，只要对象具有奔跑的能力，就可以实现这个接口，拥有奔跑的能力（方法）。所以在java中，类的单继承多实现就很好理解了。在应用场景中，抽象类用于同类事物，而接口多是可以横跨很多个类。 1.2 接口和抽象类的区别 抽象类则可以包含普通方法，接口中的普通方法默认为抽象方法。 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是 public static final 类型的，并且必须赋值，否则通不过编译。 接口不能包含构造器，抽象类可以包含构造器，抽象类里的构造器并不是用于创建对象，而是让其子类调用这些构造器来完成属于抽象类的初始化操作。 接口里不能包含初始化块，但抽象类里完全可以包含初始化块。 就是我们所说的单继承多实现了。 1.3 什么时候应该使用接口而不用抽象类 需要实现多态 要实现的方法(功能)不是当前类族的必要(属性). 要为不同类族的多个类实现同样的方法(功能). 参考资料：搞了这么多年终于知道接口和抽象类的应用场景了 2 反射机制和动态代理 2.1 反射机制 反射(Reflection)是Java程序开发语言的特征之一它允许运行中的Java程序获取自身的信息，并且可以操作类或对象的内部属性。通过反射机制，可以在运行时访问Java对象的属性，方法，构造方法等。 2.1.1 反射的应用场景 反射的主要应用场景有： 开发通用框架 - 反射最重要的用途就是开发各种通用框架。很多框架（比如 Spring）都是配置化的（比如通过 XML 文件配置 JavaBean、Filter 等），为了保证框架的通用性，它们可能需要根据配置文件加载不同的对象或类，调用不同的方法，这个时候就必须用到反射——运行时动态加载需要加载的对象。 动态代理 - 在切面编程（AOP）中，需要拦截特定的方法，通常，会选择动态代理方式。这时，就需要反射技术来实现了。 注解 - 注解本身仅仅是起到标记作用，它需要利用反射机制，根据注解标记去调用注解解释器，执行行为。如果没有反射机制，注解并不比注释更有用。 可扩展性功能 - 应用程序可以通过使用完全限定名称创建可扩展性对象实例来使用外部的用户定义类。 2.1.2 反射的缺点 性能开销 - 由于反射涉及动态解析的类型，因此无法执行某些 Java 虚拟机优化。因此，反射操作的性能要比非反射操作的性能要差，应该在性能敏感的应用程序中频繁调用的代码段中避免。 破坏封装性 - 反射调用方法时可以忽略权限检查，因此可能会破坏封装性而导致安全问题。 内部曝光 - 由于反射允许代码执行在非反射代码中非法的操作，例如访问私有字段和方法，所以反射的使用可能会导致意想不到的副作用，这可能会导致代码功能失常并可能破坏可移植性。反射代码打破了抽象，因此可能会随着平台的升级而改变行为。 2.2 动态代理 代理模式是为了提供额外或不同的操作，而插入的用来替代“实际”对象的对象，这些操作涉及到与“实际”对象的通信，因此代理通常充当中间人角色。Java的动态代理比代理的思想更前进了一步，它可以动态地创建并代理并动态地处理对所代理方法的调用。在动态代理上所做的所有调用都会被重定向到单一的调用处理器上，它的工作是揭示调用的类型并确定相应的策略。 学习Spring的时候，我们知道Spring主要有两大思想，一个是IoC，另一个就是AOP，对于IoC，它利用的是反射机制，依赖注入就不用多说了，而对于Spring的核心AOP来说，使用了动态代理，其实底层也是反射。我们不但要知道怎么通过AOP来满足的我们的功能，我们更需要学习的是其底层是怎么样的一个原理，而AOP的原理就是java的动态代理机制。 参考资料： 深入理解Java反射和动态代理 深入理解Java反射+动态代理 3 项目中事务的使用 事务的四个特征：原子性、一致性、隔离性、持久性 3.1 事务传播属性 Propagation.REQUIRED:支持当前事务,如果当前没有事务,则新建一个事务,默认使用这种,也是最常见的. Propagation.SUPPORTS:支持当前事务,如果没有事务,就以非事务的方式执行. Propagation.MANDATORY:支持当前事务,如果没有事务,就抛出异常. Propagation.REQUIRES_NEW:新建事务,如果当前存在事务,就把当前事务挂起. Propagation.NOT_SUPPORTED:以非事务的方式执行操作,如果当前存在事务,就把当前事务挂起. Propagation.NEVER:以非事务的方式执行,如果当前存在事务,则会抛出异常. Propagation.NESTED:如果当前事务存在，则执行嵌套事务，否则执行类似REQUIRED的操作. SpringBoot注解@Transactional实现事务 一个事务内部，没有隔离的概念的。打个比方：在同一个事务里，先对一条记录执行更新操作，然后再执行查询操作，整个事务没有完全的执行完毕，那么在执行查询的时候，虽然事务还没结束，但是查询的仍然是最新的更新的值，如果是另外一个事务查询这条记录，因为第一个事务并没有完全结束，所以查询到的就是老值。 3.2 @Transactional事务不生效的场景 spring的事务实现原理为AOP，只有通过代理对象调用方法才能被拦截，事务才能生效。 private、final、static方法，事务不生效，入口方法必须是public,spring的AOP特性决定的，spring认为private自己用的方法应该自己控制，不应该用事务切进去 Spring的事务管理默认只对出现运行期异常(java.lang.RuntimeException及其子类)进行回滚（至于为什么spring要这么设计：因为spring认为Checked的异常属于业务的，coder需要给出解决方案而不应该直接扔该框架） 同类调用不生效（service方法中调用本类中的另一个方法，事务没有生效）： 如果使用的是rollbakfor的默认，已检查的异常（所有派生自Error和RuntimeException的类,都是未检查异常.其余的是已检查异常， 比如nullPointException是未检查的，IllegalAccessException 是已检查的）不回滚,可设为rollbackFor={Exception.class} 最好不要把@Trasaction注解到接口上：在接口上使用 @Transactional 注解，只能当你设置了基于接口的代理时它才生效。因为注解是不能继承的，这就意味着如果正在使用基于类的代理时，那么事务的设置将不能被基于类的代理所识别，而且对象也将不会被事务代理所包装。 确认你的类是否被代理了（因为spring的事务实现原理为AOP，只有通过代理对象调用方法才能被拦截，事务才能生效 ）。 确保你的业务和事务入口在同一个线程里，否则事务也是不生效的 。 参考资料： 《java开发事务篇】之一分钟搞懂事务、使用方式和特定场景》 4 Spring IoC和AOP 4.1 IoC 控制反转 IoC 是 Inversion of Control 的简写，译为“控制反转”，它不是一门技术，而是一种设计思想，是一个重要的面向对象编程法则，能够指导我们如何设计出松耦合、更优良的程序。Spring 通过 IoC 容器来管理所有Java对象的实例化和初始化，控制对象与对象之间的依赖关系。我们将由 IoC 容器管理的 Java 对象称为 Spring Bean，它与使用关键字 new 创建的 Java 对象没有任何区别。 在传统的 Java 应用中，一个类想要调用另一个类中的属性或方法，通常会先在其代码中通过 new Object() 的方式将后者的对象创建出来，然后才能实现属性或方法的调用。为了方便理解和描述，我们可以将前者称为“调用者”，将后者称为“被调用者”。也就是说，调用者掌握着被调用者对象创建的控制权。 但在 Spring 应用中，Java 对象创建的控制权是掌握在 IoC 容器手里的，其大致步骤如下。 开发人员通过 XML 配置文件、注解、Java 配置类等方式，对 Java 对象进行定义，例如在 XML 配置文件中使用 &lt;bean&gt; 标签、在 Java 类上使用 @Component 注解等。 Spring 启动时，IoC 容器会自动根据对象定义，将这些对象创建并管理起来。这些被 IoC 容器创建并管理的对象被称为 Spring Bean。 当我们想要使用某个 Bean 时，可以直接从 IoC 容器中获取（例如通过 ApplicationContext 的 getBean() 方法），而不需要手动通过代码（例如 new Obejct() 的方式）创建。 IoC 带来的最大改变不是代码层面的，而是从思想层面上发生了“主从换位”的改变。原本调用者是主动的一方，它想要使用什么资源就会主动出击，自己创建；但在 Spring 应用中，IoC 容器掌握着主动权，调用者则变成了被动的一方，被动的等待 IoC 容器创建它所需要的对象（Bean）。 这个过程在职责层面发生了控制权的反转，把原本调用者通过代码实现的对象的创建，反转给 IoC 容器来帮忙实现，因此我们将这个过程称为 Spring 的“控制反转”。 依赖注入（Denpendency Injection，简写为 DI）是 Martin Fowler 在 2004 年在对“控制反转”进行解释时提出的。Martin Fowler 认为“控制反转”一词很晦涩，无法让人很直接的理解“到底是哪里反转了”，因此他建议使用“依赖注入”来代替“控制反转”。控制反转核心思想就是由 Spring负责对象的创建。在对象创建过程中，Spring会自动根据依赖关系，将它依赖的对象注入到当前对象中，这就是所谓的“依赖注入”。 依赖注入本质上是 Spring Bean 属性注入的一种，只不过这个属性是一个对象属性而已。 参考资料：Spring IoC（控制反转） 4.2 AOP 面向切面编程 AOP保证在不修改源代码的前提下，去为系统中的业务组件添加某种通用功能。 5 在实际工作中怎样对SQL进行调优 5.1 防止索引失效 索引会提升数据的查询效率，但是会降低“增删改”的效率。尽管如此，索引还是很划算的，因为我们大多数的操作就是查询，查询对于程序的性能影响是很大的。索引分为单值索引、唯一索引、复合索引。 在MySQL中不建议使用left join，即使on过滤条件列索引，一些情况也不会走索引，导致大量的数据行被扫描，SQL性能变得很差； 使用 != 或者 &lt;&gt;会导致索引失效，进而会全表搜索，所以如果数据量大的话，谨慎使用； 类型不一致也将导致索引失效，比如age的类型是varchar类型，那么查询的时候如果where条件设置age = 12，那么索引就会失效，但是有一种情况不会使索引失效，如果age的类型是int，查询条件age传的是varchar类型的值，是可以走索引的，因为MySQL内部做了隐式类型转换。 索引列作为函数的入参，会导致索引失效； 如果索引对列进行了四则运算（+，-，*，/，!)，都会使索引失效； OR导致索引失效，例如where user = 'zhangsan' or age = 11。但是也不是所有的OR都使索引失效，如果OR连接的是同一个字段，那么索引不会失效； 模糊查询导致索引失效； NOT IN、 NOT EXISTS这两种用法都不走索引，但是IN还是走索引的； IS NULL不走索引，IS NOT NULL走索引。 在设计字段时，如果字段没有要求‘没有值的情况下一定要设置为NULL’，那么建议设置为空字符串。 5.2 SELECT检查 尽量不使用select *，需要什么字段就取什么字段； SQL语句的SELECT后面使用自定义函数，那么SQL查询结果返回多少行，那么UDF函数就会被调用多少次，非常影响性能； 如果SELECT出现text类型的字段，就会消耗大量的网络和IO宽带，由于返回的内容过大，超过max_allowed_packet设置会导致程序报错，需要评估谨慎使用。 gorup_concat是一个字符串聚合函数，会影响SQL的响应时间，如果返回的值过大超过了max_allowed_packet设置会导致程序报错。 在select后面有子查询的情况称为内联子查询，SQL返回多少行，子查询就需要执行过多少次，严重影响SQL性能。 5.3 LIMIT 当查询我们知道只会有一条结果或者我们只需要一条结果的时候，加上limit 1可以增加性能，因为mysql数据库引擎会在找到一条数据后停止检索，而不是往后查找下一条符合条件的数据。 慎用LIMIT m,n分页查询，越往后面翻页，即m值越大的情况下SQL的耗时会越长。 6 Redis是如何实现高可用的？ Redis实现高可用的手段主要有以下四种： 数据持久化：保证了数据不丢失； Redis主从同步：让Redis从单机变成了多机。它有两种模式：主从模式和从从模式，但当主节点出现问题时，需要人工手动恢复系统； Redis哨兵模式：用来监控 Redis 主从模式，并提供了自动容灾恢复的功能。 Redis 集群：除了可以提供主从和哨兵的功能之外，还提供了多个主从节点的集群功能，这样就可以把数据均匀的存储各个主机主节点上，实现了系统的横向扩展，大大提高了 Redis 的并发处理能力。 6.1 数据持久化 数据持久化保证了系统在发生宕机或者重启之后数据不会丢失，增加了系统的可靠性和减少了系统不可用的时间（省去了手动恢复数据的过程）； 在 Redis 4.0 之前数据持久化方式有两种：AOF方式和RDB方式，在 Redis 4.0 推出了混合持久化的功能。 RDB（Redis DataBase，快照恢复）是将某一个时刻的内存数据，以二进制的方式写入磁盘。RDB 默认的保存文件为 dump.rdb，优点是以二进制存储的，因此占用的空间更小、数据存储更紧凑，并且与 AOF 相比，RDB 具备更快的重启恢复能力，但有数据丢失的风险。 AOF（Append-Only File，只追加文件）是指将所有的操作命令，以文本的形式追加到文件中。AOF 默认的保存文件为 appendonly.aof，它的优点是存储频率更高，因此丢失数据的风险就越低，并且 AOF 并不是以二进制存储的，所以可读性更高。缺点是占用空间大，重启之后的数据恢复速度比较慢。 Redis 混合持久化的存储模式指的是 Redis 可以使用 RDB + AOF 两种格式来进行数据持久化，开始的数据以 RDB 的格式进行存储，因此只会占用少量的空间，之后的命令会以 AOF 的方式进行数据追加，这样就可以减低数据丢失的风险，同时可以提高数据恢复的速度这样就可以做到扬长避短物尽其用了。 可以使用config get aof-use-rdb-preamble的命令来查询 Redis 混合持久化的功能是否开启 6.2 Redis主从同步 主从同步是 Redis 多机运行中最基础的功能，有一主节点和多个从节点，多个节点组成一个 Redis 集群，在这个集群主节点用来进行数据的操作，其他从节点用于同步主节点的内容，并且提供给客户端进行数据查询。 Redis 主从同步分为：主从模式和从从模式。 主从模式是一个主节点和多个一级从节点 从从模式是在主从模式的基础上，一级从节点下面还可以拥有更多的从节点 使用主从模式就可以实现数据的读写分离，把写操作的请求分发到主节点上，把其他的读操作请求分发到从节点上，这样就减轻了 Redis 主节点的运行压力，并且提高了 Redis 的整体运行速度。不但如此使用主从模式还实现了 Redis 的高可用，当主服务器宕机之后，可以很迅速的把从节点提升为主节点，为 Redis 服务器的宕机恢复节省了宝贵的时间。并且主从复制还降低了数据丢失的风险，因为数据是完整拷贝在多台服务器上的，当一个服务器磁盘坏掉之后，可以从其他服务器拿到完整的备份数据。 6.3 哨兵模式 Redis 主从同步有那么多的优点，但是有一个致命的缺点，就是当 Redis 的主节点宕机之后，必须人工介入手动恢复，如果半夜突然发生主节点宕机的问题，此时如果等待人工去处理就会很慢，这个时候我们就需要用到 Redis 的哨兵模式了。 Redis 哨兵模式就是用来监视 Redis 主从服务器的，当 Redis 的主从服务器发生故障之后，Redis 哨兵提供了自动容灾修复的功能。 Redis哨兵模块存储在Redis应用根目录下的 src/redis-sentinel 目录下，我们可以使用命令./src/redis-sentinel sentinel.conf来启动哨兵功能。 工作原理: 哨兵的工作原理是每个哨兵会以每秒钟 1 次的频率，向已知的主服务器和从服务器，发送一个 PING 命令。如果最后一次有效回复 PING 命令的时间，超过了配置的最大下线时间（Down-After-Milliseconds）时，默认是 30s，那么这个实例会被哨兵标记为主观下线。如果一个主服务器被标记为主观下线，那么正在监视这个主服务器的所有哨兵节点，要以每秒 1 次的频率确认主服务器是否进入了主观下线的状态。如果有足够数量（quorum 配置值）的哨兵证实该主服务器为主观下线，那么这个主服务器被标记为客观下线。此时所有的哨兵会按照规则（协商）自动选出新的主节点服务器，并自动完成主服务器的自动切换功能，而整个过程都是无须人工干预的。 6.4 Redis 集群 Redis集群也就是 Redis Cluster，它是 Redis 3.0 版本推出的 Redis 集群方案，有多个主节点同时每个主节点有多个从节点，将数据分布在不同的主服务器上，以此来降低系统对单主节点的依赖，并且可以大大提高 Redis 服务的读写性能。Redis 集群除了拥有主从模式 + 哨兵模式的所有功能之外，还提供了多个主从节点的集群功能，实现了真正意义上的分布式集群服务。 Redis 集群可以实现数据分片服务，也就是说在 Redis 集群中有 16384 个槽位用来存储所有的数据，当我们有 N 个主节点时，可以把 16384 个槽位平均分配到 N 台主服务器上。当有键值存储时，Redis 会使用 crc16 算法进行 hash 得到一个整数值，然后用这个整数值对 16384 进行取模来得到具体槽位，再把此键值存储在对应的服务器上，读取操作也是同样的道理，这样我们就实现了数据分片的功能。 参考资料：Redis是如何实现高可用的？ 7 利用JDK，不依赖外部工具，实现一个简单的缓存机制 7.1 存储集合的选择 实现本地缓存，存储容器肯定是 key/value 形式的数据结构，在 Java 中，也就是我们常用的 Map 集合。Map 中有 HashMap、Hashtable、ConcurrentHashMap 几种供我们选择，如果不考虑高并发情况下数据安全问题，我们可以选择HashMap，如果考虑高并发情况下数据安全问题，我们可以选择 Hashtable、ConcurrentHashMap 中的一种集合，但是我们优先选择 ConcurrentHashMap，因为 ConcurrentHashMap 的性能比 Hashtable 要好。 7.2 过期缓存处理 因为缓存直接存储在内存中，如果我们不处理过期缓存，内存将被大量无效缓存占用，这不是我们想要的，所以我们需要清理这些失效的缓存。过期缓存处理可以参考 Redis 的策略来实现，Redis 采用的是定期删除 + 懒惰淘汰策略。 7.2.1 定期删除策略 定期删除策略是每隔一段时间检测已过期的缓存，并且降之删除。这个策略的优点是能够确保过期的缓存都会被删除。同时也存在着缺点，过期的缓存不一定能够及时的被删除，这跟我们设置的定时频率有关系，另一个缺点是如果缓存数据较多时，每次检测也会给 cup 带来不小的压力。 7.2.2 懒惰淘汰策略 懒惰淘汰策略是在使用缓存时，先判断缓存是否过期，如果过期将它删除，并且返回空。这个策略的优点是只有在查找的时候，才判断是否过期，对 CUP 影响较。同时这种策略有致命的缺点，当存入了大量的缓存，这些缓存都没有被使用并且已过期，都将成为无效缓存，这些无效的缓存将占用你大量的内存空间，最后导致服务器内存溢出。 我们简单的了解了一下 Redis 的两种过期缓存处理策略，每种策略都存在自己的优缺点。所以我们在使用过程中，可以将两种策略组合起来，结合效果还是非常理想的。 7.3 缓存淘汰策略 缓存淘汰跟过期缓存处理要区别开来，缓存淘汰是指当我们的缓存个数达到我们指定的缓存个数时，毕竟我们的内存不是无限的。如果我们需要继续添加缓存的话，我们就需要在现有的缓存中根据某种策略淘汰一些缓存，给新添加的缓存腾出位置，下面一起来认识几种常用的缓存淘汰策略。 7.3.1 先进先出策略 最先进入缓存的数据在缓存空间不够的情况下会被优先被清除掉，以腾出新的空间接受新的数据。该策略主要比较缓存元素的创建时间。在一些对数据实效性要求比较高的场景下，可考虑选择该类策略，优先保障最新数据可用。 7.3.2 最少使用策略 无论是否过期，根据元素的被使用次数判断，清除使用次数较少的元素释放空间。该策略主要比较元素的hitCount（命中次数），在保证高频数据有效性场景下，可选择这类策略。 7.3.3 最近最少使用策略 无论是否过期，根据元素最后一次被使用的时间戳，清除最远使用时间戳的元素释放空间。该策略主要比较缓存最近一次被get使用时间。在热点数据场景下较适用，优先保证热点数据的有效性。 7.3.4 随机淘汰策略 无论是否过期，随机淘汰某个缓存，如果对缓存数据没有任何要求，可以考虑使用该策略。 7.3.5 不淘汰策略 当缓存达到指定值之后，不淘汰任何缓存，而是不能新增缓存，直到有缓存淘汰时，才能继续添加缓存。 参考资料：实现 Java 本地缓存，该从这几点开始 8 Exception和Error的区别 Exception和Error都继承了Throwable类。 Exception 是程序正常运行中，可以预料的意外情况，可能并且应该被捕获，进行相应处理；Error 是指在正常情况下，不大可能出现的情况，绝大部分的 Error 都会导致程序（比如 JVM 自身）处于非正常的、不可恢复状态 Exception 又分为cheked exception（检查异常） 和 unchecked exception（不检查异常）。 checked exception 指的是在源代码里必须处理的异常。例如： IOException unchecked exception 指的是不用在源代码里处理，为运行时异常。例如： NullPointerException ClassCastException 绝大数的Error会导致程序处于非正常状态。例如： NoClassDefFoundError OutOfMemoryError StackOverflowError 参考资料： 《谈谈你对Exception 和 Error的理解》 《Exception和Error有什么区别吗》 9 设计模式 23种经典设计模式，设计模式的具逻辑和实现，可移步菜鸟教程了解。 9.1 创建型模式 这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。 工厂模式（Factory Pattern） 抽象工厂模式（Abstract Factory Pattern） 单例模式（Singleton Pattern） 建造者模式（Builder Pattern） 原型模式（Prototype Pattern） 9.2 结构型模式 这些设计模式关注类和对象的组合。继承的概念被用来组合接口和定义组合对象获得新功能的方式。 适配器模式（Adapter Pattern） 桥接模式（Bridge Pattern） 过滤器模式（Filter、Criteria Pattern） 组合模式（Composite Pattern） 装饰器模式（Decorator Pattern） 外观模式（Facade Pattern） 享元模式（Flyweight Pattern） 代理模式（Proxy Pattern） 9.3 行为型模式 这些设计模式特别关注对象之间的通信。 责任链模式（Chain of Responsibility Pattern） 命令模式（Command Pattern） 解释器模式（Interpreter Pattern） 迭代器模式（Iterator Pattern） 中介者模式（Mediator Pattern） 备忘录模式（Memento Pattern） 观察者模式（Observer Pattern） 状态模式（State Pattern） 空对象模式（Null Object Pattern） 策略模式（Strategy Pattern） 模板模式（Template Pattern） 访问者模式（Visitor Pattern） 9.4 J2EE 模式 这些设计模式特别关注表示层。这些模式是由 Sun Java Center 鉴定的。 MVC 模式（MVC Pattern） 业务代表模式（Business Delegate Pattern） 组合实体模式（Composite Entity Pattern） 数据访问对象模式（Data Access Object Pattern） 前端控制器模式（Front Controller Pattern） 拦截过滤器模式（Intercepting Filter Pattern） 服务定位器模式（Service Locator Pattern） 传输对象模式（Transfer Object Pattern） 10 一个线程调用两次start()方法会发生什么？简单谈谈线程的几种状态 10.1 调用两次start() Java的线程是不允许启动两次的，第二次调用必然会抛岀IllegalThreadStateEXception，这是一种运行时异常，多次调用 start 被认为是编程错误。 10.2 线程的生命周期（线程状态） 新建（NEW），表示线程被创建出来还没真正启动的状态，可以认为它是个Java内部状态。 就绪（ RUNNABLE），表示该线程已经在WM中执行，当然由于执行需要计算资源，它可能是正在运行，也可能还在等待系统分配给它cpu片段，在就绪队列里面排队。 运行（Running）在其他一些分析中，会额外区分一种状态 RUNNING，但是从 Java API的角度，并不能表示出来。 阻塞（ BLOCKED），这个状态和我们前面两讲介绍的同步非常相关，阻塞表示线程在等待 Monitor lock。比如，线程试图通过synchronized去获取某个锁，但是其他线程已经独占了，那么当前线程就会处于阻塞状态。 等待（ WAITING），表示正在等待其他线程釆取某些操作。一个常见的场景是类似生产者消费者模式，发现任务条件尚未满足，就让当前消费者线程等待（wait），另外的生产者线程去准备任务数据，然后通过类似 notify等动作，通知消费线程可以继续工作了。 计时等待（ TIMED_WAIT），其进入条件和等待状态类似，但是调用的是存在超时条件的方法，比如wait或join等方法的指定超时版本，如下面示例 1public final native void wait（long timeout） throws InterruptedException； 终止（ TERMINATED），不管是意外退出还是正常执行结束，线程已经完成使命，终止运行，也有人把这个状态叫作死亡在第二次调用 start()方法的时候，线程可能处于终止或者其他（非NEW）状态，但是不论如何，都是不可以再次启动的。 参考资料：https://cloud.tencent.com/developer/article/1625433 11 RocketMQ RocketMQ是一个纯Java、分布式队列模型的消息中间件，具有高可用、高可靠、高实时、低延迟的特点。 11.1 消息中间件的通用功能 业务解耦：这也是发布订阅的消息模型。生产者发送指令到MQ中，然后下游订阅这类指令的消费者会收到这个指令执行相应的逻辑，整个过程与具体业务无关，抽象成了一个发送指令，存储指令，消费指令的过程。 前端削峰：前端发起的请求在短时间内太多后端无法处理，可以堆积在MQ中，后端按照一定的顺序处理，秒杀系统就是这么实现的。 11.2 RocketMQ的特点 亿级消息的堆积能力：单个队列中的百万级消息的累积容量。 高可用性 高可靠性 支持分布式事务消息 支持顺序消息：消息在Broker中是采用队列的FIFO模式存储的，也就是发送是顺序的，只要保证消费的顺序性即可。 支持消息过滤：建议采用消费者业务端的tag过滤。 支持定时消息和延迟消息 11.3 RocketMQ消息模型 Message 就是要传输的消息，一个消息必须有一个主题，一条消息也可以有一个可选的Tag（标签）和额外的键值对，可以用来设置一个业务的key，便于开发中在broker服务端查找消息。 Topic 主题，是消息的第一级类型，每条消息都有一个主题，就像信件邮寄的地址一样。主题就是我们具体的业务，比如一个电商系统可以有订单消息，商品消息，采购消息，交易消息等。Topic和生产者和消费者的关系非常松散，生产者和Topic可以是1对多，多对1或者多对多，消费者也是这样。 Tag 标签，是消息的第二级类型，可以作为某一类业务下面的二级业务区分，它的主要用途是在消费端的消息过滤。比如采购消息分为采购创建消息，采购审核消息，采购推送消息，采购入库消息，采购作废消息等，这些消息是同一Topic和不同的Tag，当消费端只需要采购入库消息时就可以用Tag来实现过滤，不是采购入库消息的tag就不处理。 Group 组，可分为ProducerGroup生产者组合ConsumerGroup消费者组，一个组可以订阅多个Topic。一般来说，某一类相同业务的生产者和消费者放在一个组里。 Message Queue 消息队列，一个Topic可以划分成多个消息队列。Topic只是个逻辑上的概念，消息队列是消息的物理管理单位，当发送消息的时候，Broker会轮询包含该Topic的所有消息队列，然后将消息发出去。有了消息队列，可以使得消息的存储可以分布式集群化，具有了水平的扩展能力。 offset 是指消息队列中的offset，可以认为就是下标，消息队列可看做数组。offset是java long，64位，理论上100年不会溢出，所以可以认为消息队列是一个长度无限的数据结构。 参考资料：RocketMq面试题精选 12 RocketMQ和其他消息中间件的区别 13 序列化和反序列化 13.1 什么是序列化和反序列化？ 序列化：就是将对象转化成字节序列的过程。 反序列化：就是将字节序列转化成对象的过程。 对象序列化成的字节序列会包含对象的类型信息、对象的数据等，说白了就是包含了描述这个对象的所有信息，能根据这些信息“复刻”出一个和原来一模一样的对象。 13.2 为什么要序列化和反序列化？ 持久化：对象是存储在JVM中的堆区的，但是如果JVM停止运行了，对象也不存在了。序列化可以将对象转化成字节序列，可以写进硬盘文件中实现持久化。在新开启的JVM中可以读取字节序列进行反序列化成对象。 网络传输：网络直接传输数据，但是无法直接传输对象，可在传输前序列化，传输完成后反序列化成对象。所以所有可在网络上传输的对象都必须是可序列化的。 13.3 实现序列化和反序列化的方法 Java为我们提供了对象序列化的机制，规定了要实现序列化对象的类要满足的条件和实现方法。 对于要序列化对象的类要去实现Serializable接口或者Externalizable接口。 实现方法：JDK提供的ObjectOutputStream和ObjectInputStream来实现序列化和反序列化。 13.4 serialVersionUID的作用 先讲述下序列化的过程：在进行序列化时，会把当前类的serialVersionUID写入到字节序列中（也会写入序列化的文件中），在反序列化时会将字节流中的serialVersionUID同本地对象中的serialVersionUID进行对比，一直的话进行反序列化，不一致则失败报错（报InvalidCastException异常） serialVersionUID的生成有三种方式（private static final long serialVersionUID= XXXL ）： 显式声明：默认的1L 显式声明：根据包名、类名、继承关系、非私有的方法和属性以及参数、返回值等诸多因素计算出的64位的hash值 隐式声明：未显式的声明serialVersionUID时java序列化机制会根据Class自动生成一个serialVersionUID（最好不要这样，因为如果Class发生变化，自动生成的serialVersionUID可能会随之发生变化，导致匹配不上） 序列化类增加属性时，最好不要修改serialVersionUID，避免反序列化失败 参考资料：一文搞懂序列化与反序列化 14 Dubbo和SpringCloud核心组件Feign、Ribbin、Hystrix对比 14.1 协议 Dubbo：支持多传输协议：Dubbo、Rmi、http，可灵活配置。 Feign：基于http传输协议，短连接，性能比Dubbo低。 14.2 与SpringCloud集成 Dubbo：在早期Dubbo是与Spring Cloud 集成有一些脱落,但是在Spring Cloud Alibaba 出现后，spring-cloud-starter-dubbo 与Spring Cloud完美集成 Feign：Spring Cloud 最早支持的RPC框架，兼容性好 14.3 负载均衡 Dubbo内置了如下负载均衡算法，用户可直接配置使用： 算法 特性 备注 RandomLandBalance 加权随机 默认算法，默认权重相同 RoundRobinLoadBalance 加权轮询 借鉴与Nginx的平滑加权轮询算法，默认权重相同 LeastActiveLoadBalance 最少活跃优先+加权随机 背后是能者多劳的思想 ShortestResponseLoadBalance 最短响应优先+加权随机 更加关注响应速度 ConsistentHashLoadBalance 一致性Hash 确定的入参，确定的提供者，适用于有状态的请求 同时支持服务端负载均衡和客户端负载均衡配置，灵活度非常高 Fegin自身是没有负载均衡能力的，之前默认使用Ribbon作为负载均衡的组件，但是Netfix已经不再维护。新版本的Spring Cloud已经将Ribbon替换成Spring Cloud Load Balancer,Ribbon是客户端级别的负载均衡，不像dubbo支持客户端和服务端双向配置 14.4 容错机制 Dubbo支持多种容错策略： Failover Cluste：失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries="2" 来设置重试次数(不含第一次)。默认容错机制 Failfast Cluster：快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster：失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster ：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster：并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks="2" 来设置最大并行数。 Broadcast Cluster：广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 Feign Feign默认使用Netfix Hystrix作为服务熔断的组件。Hystix提供了服务降级，服务熔断，依赖隔离，监控（Hystrix Dashboard）等功能。 目前 Hystrix 已经处于维护（maintenance）状态，不再继续开发，这个消息是Netfix 在2018年对外宣布的。Hystrix GitHub页面上也说明了该项目目前处于维护模式 在使用dubbo后续的熔断降级也还可以使用Alibaba Sentinel 14.5 迁移 Dubbo dubbo为了提供对RestTemplate和OpenFeign客户端端的支持，在Dubbo Spring Cloud提供了@DubboTransported注解，使客户端无需额外处理即可兼容RestTemplate和OpenFeign的调用，换而言之在不调整 Feign 接口以及 RestTemplate URL 的前提下，可以实现无缝迁移 比如在客户端使用OpenFeign调用duboo服务，只需要添加如下注解如下 12@FeignClient("order")@DubboTransported(protocol = "dubbo") 使用RestTemplate或OpenFeign调用Dubbo服务会经历以下过程： 根据服务名得到注册中心的Dubbo服务DubboMetadataService。 使用DubboMetadataService里提供的getServiceRestMetadata方法获取要使用的Dubbo服务和对应的Rest元数据。 基于Dubbo服务和Rest元数据构造GenericService。 服务调用过程中使用GenericService发起泛化调用。 Feign 没有提供对dubbo无缝迁移的支持 参考资料：Spring Cloud RPC(Feign VS Dubbo)多维度对比选型 15 线程的创建方式 15.1 继承Thread类 12345678910111213public class App &#123; public static void main(String[] args)&#123; new ThreadObject().start(); &#125; static class ThreadObject extends Thread &#123; @Override public void run() &#123; System.out.println("current thread name is :" + Thread.currentThread().getName()); &#125; &#125;&#125; 15.2 实现Runnable接口 1234567891011121314public class App &#123; public static void main(String[] args)&#123; Thread thread = new Thread(new RunnableObject()); thread.start(); &#125; static class RunnableObject implements Runnable &#123; @Override public void run() &#123; System.out.println("current thread name is :" + Thread.currentThread().getName()); &#125; &#125;&#125; 15.3 实现Callable接口 1234567891011121314151617181920212223242526public class App &#123; public static void main(String[] args) &#123; //执行Callable方式，需要FutureTask实现，用于接收运算结果 FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(new CallableObject()); new Thread(futureTask).start(); try &#123; // 获取线程执行完毕后的返回值 Integer sum = futureTask.get(); System.out.println("sum is :" + sum); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; // Callable泛型定义返回值的类型 static class CallableObject implements Callable&lt;Integer&gt; &#123; public Integer call() throws Exception &#123; Integer sum = 0; for (int i = 0; i &lt; 10; i++) &#123; sum += i; &#125; return sum; &#125; &#125;&#125; 15.4 基于线程池创建 123456789101112131415161718public class App &#123; public static void main(String[] args) &#123; //创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for(int i = 0 ; i &lt; 10 ; i++) &#123; //调用execute()方法创建线程 //采用匿名内部类的方法，创建Runnable对象，并重写run()方法 threadPool.execute(new Runnable() &#123; public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125;); &#125; // 关闭线程池 threadPool.shutdown(); &#125;&#125; 未完，待续...]]></content>
      <categories>
        <category>代码人生</category>
      </categories>
      <tags>
        <tag>Ioc和AOP</tag>
        <tag>反射</tag>
        <tag>动态代理</tag>
        <tag>异常</tag>
        <tag>索引</tag>
        <tag>Redis</tag>
        <tag>缓存</tag>
        <tag>设计模式</tag>
        <tag>Dubbo</tag>
        <tag>SpringCloud</tag>
        <tag>序列化</tag>
        <tag>线程</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList.retainAll()方法解析]]></title>
    <url>%2F2023%2F03%2F09%2FArrayList-retainAll-%E6%96%B9%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[在工作中，用java.util.ArrayList.retainAll(Collection&lt;?&gt;)方法判断两个list集合是否有交集（两个list是否有相同的元素）。如果两个集合有相同元素，那么retainAll返回true。但是如果两个集合的元素完全相同，返回的结果却是false,而如果两个list集合的元素都不一样，retainAll却返回true。 这是怎么回事呢？ 先来看看ArrayList.retainAll(Collection&lt;?&gt;) 源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Retains only the elements in this list that are contained in the * specified collection. In other words, removes from this list all * of its elements that are not contained in the specified collection. * * @param c collection containing elements to be retained in this list * @return &#123;@code true&#125; if this list changed as a result of the call * @throws ClassCastException if the class of an element of this list * is incompatible with the specified collection * (&lt;a href="Collection.html#optional-restrictions"&gt;optional&lt;/a&gt;) * @throws NullPointerException if this list contains a null element and the * specified collection does not permit null elements * (&lt;a href="Collection.html#optional-restrictions"&gt;optional&lt;/a&gt;), * or if the specified collection is null * @see Collection#contains(Object) */public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); return batchRemove(c, true);&#125;private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123; final Object[] elementData = this.elementData; int r = 0, w = 0; boolean modified = false; try &#123; for (; r &lt; size; r++) if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; &#125; finally &#123; // Preserve behavioral compatibility with AbstractCollection, // even if c.contains() throws. if (r != size) &#123; System.arraycopy(elementData, r, elementData, w, size - r); w += size - r; &#125; if (w != size) &#123; // clear to let GC do its work for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w; size = w; modified = true; &#125; &#125; return modified;&#125; retainAll注释的第一句已经基本交代了方法的功能 Retains only the elements in this list that are contained in the specified collection. In other words, removes from this list all of its elements that are not contained in the specified collection. 仅保留此列表中包含在指定集合中的元素。换句话说，从此列表中删除未包含在指定集合中的所有元素。 123for (; r &lt; size; r++) if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; 遍历此列表 elementData 中的元素，判断是否与 c集合有相同元素，把相同的元素覆盖在此列表的第一个地址上，以此类推。以w 记录相同元素的个数。 回到刚开始的问题上，为什么两个集合的元素都一样，结果返回 false， 两个集合元素都不一样，结果返回 true 呢？ 12345678if (w != size) &#123; // clear to let GC do its work for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w; size = w; modified = true;&#125; 如果两个集合的元素完全一样，w == size，这样会跳过判断，直接返回 modified 的初始值 false; 如果两个集合的元素都不一样，w != size满足条件，modified 被赋值为true，返回结果即为true。 所以单纯用ArrayList.retainAll()方法，根据返回值true/false,判断两个集合是否有相同元素，是不准确的。 可以使用另一个方法 java.util.Collections.disjoint(Collection&lt;?&gt; c1, Collection&lt;?&gt; c2)，两个指定collection中没有相同的元素，则返回true。如果其中一个集合为null，则抛出NullPointerException。 源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * Returns &#123;@code true&#125; if the two specified collections have no * elements in common. * * &lt;p&gt;Care must be exercised if this method is used on collections that * do not comply with the general contract for &#123;@code Collection&#125;. * Implementations may elect to iterate over either collection and test * for containment in the other collection (or to perform any equivalent * computation). If either collection uses a nonstandard equality test * (as does a &#123;@link SortedSet&#125; whose ordering is not &lt;em&gt;compatible with * equals&lt;/em&gt;, or the key set of an &#123;@link IdentityHashMap&#125;), both * collections must use the same nonstandard equality test, or the * result of this method is undefined. * * &lt;p&gt;Care must also be exercised when using collections that have * restrictions on the elements that they may contain. Collection * implementations are allowed to throw exceptions for any operation * involving elements they deem ineligible. For absolute safety the * specified collections should contain only elements which are * eligible elements for both collections. * * &lt;p&gt;Note that it is permissible to pass the same collection in both * parameters, in which case the method will return &#123;@code true&#125; if and * only if the collection is empty. * * @param c1 a collection * @param c2 a collection * @return &#123;@code true&#125; if the two specified collections have no * elements in common. * @throws NullPointerException if either collection is &#123;@code null&#125;. * @throws NullPointerException if one collection contains a &#123;@code null&#125; * element and &#123;@code null&#125; is not an eligible element for the other collection. * (&lt;a href="Collection.html#optional-restrictions"&gt;optional&lt;/a&gt;) * @throws ClassCastException if one collection contains an element that is * of a type which is ineligible for the other collection. * (&lt;a href="Collection.html#optional-restrictions"&gt;optional&lt;/a&gt;) * @since 1.5 */public static boolean disjoint(Collection&lt;?&gt; c1, Collection&lt;?&gt; c2) &#123; // The collection to be used for contains(). Preference is given to // the collection who's contains() has lower O() complexity. Collection&lt;?&gt; contains = c2; // The collection to be iterated. If the collections' contains() impl // are of different O() complexity, the collection with slower // contains() will be used for iteration. For collections who's // contains() are of the same complexity then best performance is // achieved by iterating the smaller collection. Collection&lt;?&gt; iterate = c1; // Performance optimization cases. The heuristics: // 1. Generally iterate over c1. // 2. If c1 is a Set then iterate over c2. // 3. If either collection is empty then result is always true. // 4. Iterate over the smaller Collection. if (c1 instanceof Set) &#123; // Use c1 for contains as a Set's contains() is expected to perform // better than O(N/2) iterate = c2; contains = c1; &#125; else if (!(c2 instanceof Set)) &#123; // Both are mere Collections. Iterate over smaller collection. // Example: If c1 contains 3 elements and c2 contains 50 elements and // assuming contains() requires ceiling(N/2) comparisons then // checking for all c1 elements in c2 would require 75 comparisons // (3 * ceiling(50/2)) vs. checking all c2 elements in c1 requiring // 100 comparisons (50 * ceiling(3/2)). int c1size = c1.size(); int c2size = c2.size(); if (c1size == 0 || c2size == 0) &#123; // At least one collection is empty. Nothing will match. return true; &#125; if (c1size &gt; c2size) &#123; iterate = c2; contains = c1; &#125; &#125; for (Object e : iterate) &#123; if (contains.contains(e)) &#123; // Found a common element. Collections are not disjoint. return false; &#125; &#125; // No common elements were found. return true;&#125;]]></content>
      <categories>
        <category>代码人生</category>
      </categories>
      <tags>
        <tag>ArrayList</tag>
        <tag>retainAll()</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析Field-injection-is-not-recommended]]></title>
    <url>%2F2023%2F02%2F22%2F%E6%B5%85%E6%9E%90Field-injection-is-not-recommended%2F</url>
    <content type="text"><![CDATA[IDEA运行SpringBoot项目，遇到以下有关 @Autowired 注解的警告：Field injection is not recommended . 这篇文章浅析这个问题，为什么会有这样的提示？为什么字段注入的方式不推荐？ 当前的 spring framework (5.0.3) 文档仅定义了两种主要的注入类型1 DI exists in two major variants: Constructor-based dependency injection and Setter-based dependency injection. 基于构造函数的依赖注入:在基于构造函数的依赖注入中，类构造函数被注释@Autowired并包含可变数量的参数以及要注入的对象。 123456789101112public class SimpleMovieLister &#123; // the SimpleMovieLister has a dependency on a MovieFinder private final MovieFinder movieFinder; // a constructor so that the Spring container can inject a MovieFinder public SimpleMovieLister(MovieFinder movieFinder) &#123; this.movieFinder = movieFinder; &#125; // business logic that actually uses the injected MovieFinder is omitted...&#125; 基于构造函数的注入的主要优点是您可以将注入的字段声明为final，因为它们将在类实例化期间启动。这对于所需的依赖项很方便。 基于Setter的依赖注入:在基于 setter 的依赖注入中，setter 方法用@Autowired. 一旦使用无参数构造函数或无参数静态工厂方法实例化 Bean，Spring 容器将调用这些 setter 方法以注入 Bean 的依赖项。 123456789101112public class SimpleMovieLister &#123; // the SimpleMovieLister has a dependency on the MovieFinder private MovieFinder movieFinder; // a setter method so that the Spring container can inject a MovieFinder public void setMovieFinder(MovieFinder movieFinder) &#123; this.movieFinder = movieFinder; &#125; // business logic that actually uses the injected MovieFinder is omitted...&#125; 但实际上还有第三种，也是被广泛应用的 基于字段的依赖注入:在基于字段的依赖注入中，字段/属性用@Autowired. 实例化类后，Spring 容器将设置这些字段。 1234567@Componentpublic class ConstructorBasedInjection &#123; @Autowired private InjectedBean injectedBean;&#125; 这是注入依赖项的最简洁的方法，因为它避免了添加样板代码，并且无需为类声明构造函数。代码看起来不错，简洁明了，但正如代码检查员已经提示我们的那样，这种方法有一些缺点。 那么为什么不推荐使用基于字段的依赖注入？ 基于字段的依赖注入缺点2 不允许不可变字段声明 基于字段的依赖注入不适用于声明为 final/immutable 的字段，因为这些字段必须在类实例化时实例化。声明不可变依赖项的唯一方法是使用基于构造函数的依赖项注入。 违反了单一责任原则 如您所知，在面向对象的计算机编程中，SOLID3首字母缩略词定义了五个设计原则，这些原则将使您的代码易于理解、灵活和可维护。SOLID中的S代表单一职责原则，这意味着一个类应该只负责软件应用程序功能的单个部分，并且它的所有服务都应该与该职责严格对齐。 使用基于字段的依赖注入，很容易在你的类中有很多依赖，一切看起来都很好。如果改为使用基于构造函数的依赖注入，随着更多的依赖项被添加到你的类中，构造函数变得越来越大。拥有一个包含十个以上参数的构造函数是一个明显的标志，表明该类有太多协作者，这可能是开始将类拆分为更小且更易于维护的部分的好时机。 这里要说明：基于构造函数依赖注入，并不说能够解决类里面的过多依赖的问题。而是说能够直观的提示我们：这个类被注入了太多的依赖，你该停下来，优化并拆分你的业务逻辑了！ 因此，尽管字段注入并不直接导致打破单一责任原则，但它肯定有助于隐藏信号，否则这些信号会非常明显。 与依赖注入容器紧密耦合 使用基于字段的注入的主要原因是避免getter和setter的样板代码或为您的类创建构造函数。最后，这意味着可以设置这些字段的唯一方法是通过Spring容器实例化类并使用反射注入它们，否则字段将保持为 null 并且您的类将被破坏/无用。 依赖注入设计模式将类依赖的创建与类本身分开，将此责任转移到类注入器，允许程序设计松散耦合并遵循单一职责和依赖倒置原则（再次是SOLID）。因此，最终通过自动装配其字段实现的类解耦会因再次与类注入器（在本例中为 Spring）耦合而丢失，从而使该类在 Spring 容器之外无用。 这意味着如果你想在应用程序容器之外使用你的类，例如用于单元测试，你必须使用 Spring 容器来实例化你的类，因为没有其他可能的方法（除了反射）来设置自动装配的字段。 隐藏的依赖 使用依赖注入模式时，受影响的类应该使用公共接口清楚地公开这些依赖关系，方法是在构造函数中公开所需的依赖关系，或者使用方法（setter）公开可选的依赖关系。当使用基于字段的依赖注入时，该类本质上将这些依赖隐藏到外部世界。 结论 我们已经看到应该尽可能避免基于字段的注入，因为它有许多缺点，无论它看起来多么优雅。推荐的方法是使用基于构造函数和基于设置器的依赖注入。对于必需的依赖项，建议使用基于构造函数的注入，以允许它们不可变并防止它们为空。对于可选的依赖项，建议使用基于 Setter 的注入。 补充 2023-02-27 构造器依赖注入，如果要注入的属性太多，构造方法会很臃肿，可以在类上加 @RequiredArgsConstructor 注解，这个注解会把final修饰的（或者@NonNull注解的）属性构建默认的构造方法。 12345678@RequiredArgsConstructorpublic class SimpleMovieLister &#123; // the SimpleMovieLister has a dependency on a MovieFinder private final MovieFinder movieFinder; // business logic that actually uses the injected MovieFinder is omitted...&#125; 从Spring Framework 4.3开始，如果目标bean只定义了一个构造函数，则不再需要在这样的构造函数上使用@Autowired注释。但是，如果有几个可用的构造函数，至少必须用@Autowired注释其中一个，以便指示容器使用哪个构造函数4。 《Spring Framework Documentation 1.4.1. Dependency Injection》↩︎ 《Field injection is not recommended – Spring IOC》↩︎ 《Wiki SOLID》↩︎ 《Core Technology:1.9.2. Using @Autowired》↩︎]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>field injection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日常工作中Java.Stream流操作汇总]]></title>
    <url>%2F2023%2F02%2F08%2F%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E4%B8%ADJava-Stream%E6%B5%81%E6%93%8D%E4%BD%9C%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[Java 8 API添加了一个新的抽象称为流Stream，以一种声明的方式处理数据。Stream 使用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对 Java 集合运算和表达的高阶抽象。Stream API可以极大提高Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。这种风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。 ————《菜鸟教程》 本片文章记录了工作中常用的Stream流操作，方便之后回顾。 1.对List集合，取User中的一个元素，形成新的List集合 12// List&lt;User&gt; userList;List&lt;String&gt; userNameList = userList.stream.map(User::getUserName).collect(Collectors.toList); 2.去重 对集合 List userList 内的对象去重（实体类User 使用Lombok 插件的@Data 注解，自动覆写 equals 和 hashCode 方法） 12// List&lt;User&gt; userList;List&lt;User&gt; newUserList = userList.stream().distinct().collect(Collectors.toList()); 3.过滤 过滤集合 List&lt;SysOptionData&gt; optionDataList 对象label为 "zhangsan" 的数据 12345678List&lt;SysOptionData&gt; resultList = optionDataList.stream().filter( optionData -&gt; "zhangsan".equals(optionData.getOptionLabel())).collect(Collectors.toList());// 返回过滤后的集合第一条数据List&lt;SysOptionData&gt; resultList = optionDataList.stream().filter( optionData -&gt; "zhangsan".equals(optionData.getOptionLabel())).findFirst(); 4.List集合转String字符串 12345// List&lt;User&gt; 包含User实体的集合，只提取username拼成一个字符串，以“，”隔开String username = userList.stream().map(User::getUsername()).collect(Collectors.joining(",")); // List&lt;String&gt; usernameListString username = usernameList.stream().collect(Collectors.joining(",")); 5.匹配两个List集合，返回新的List集合 比如有两个集合，List&lt;User&gt;、 List&lt;Address&gt; List&lt;User&gt; 集合的数据 name age address zhangsan 15 null tom 16 null jom 20 null List&lt;Address&gt; 集合的数据 name address zhangsan China tom USA jary Japan 对比着两个集合，以List&lt;User&gt;为主体，按照name字段匹配List&lt;Address&gt;集合，把匹配到的Address对象的address字段的值设置到User对象的address字段 123456789101112131415List&lt;User&gt; list = userList.stream() .map(u -&gt; &#123; return addrList.stream() .filter(a -&gt; &#123; return a.getName().equals(u.getName()); &#125;).map(a -&gt; &#123; u.setAddress(a.getAddress()); return u; &#125;).collect(Collectors.toList()); &#125;) .flatMap(List::stream) .collect(Collectors.toList());for (User user : list) &#123; System.out.println(user.toString());&#125; 结果输出： 12User [name=zhangsan, age=15, address=China]User [name=tom, age=16, address=USA] 需要注意的是，结果只会返回匹配到的数据 6.Stream map和flatMap的区别 6.1 stream.map Returns a stream consisting of the results of applying the givenfunction to the elements of this stream. 返回一个流，由将给定函数应用于该流的元素的结果组成。 示例： 12List&lt;String&gt; nameList = Stream.of("ZhangSan", "Tom").collect(Collectors.toList());nameList.stream().map(n -&gt; n + ", welcome").forEach(e -&gt; System.out.println(e)); 输出： 12ZhangSan, welcomeTom, welcome 请注意另一种情况： 12345678List&lt;String&gt; nameList = Stream.of("ZhangSan", "Tom").collect(Collectors.toList());List&lt;String[]&gt; list = nameList.stream().map(n -&gt; n.split("")).collect(Collectors.toList());for (String[] strings : list) &#123; for (int i = 0; i &lt; strings.length; i++) &#123; System.out.print(strings[i] + " "); &#125; System.out.println();&#125; 输出： 12Z h a n g S a n T o m map操作就是把一种操作运算，映射到一个序列的每一个元素上。以每个元素为一个单位，运算的结果也是相互独立的，所以返回的是List&lt;String[]&gt;，而不是List&lt;String&gt; 6.2 stream.flatMap Returns a stream consisting of the results of replacing each element ofthis stream with the contents of a mapped stream produced by applyingthe provided mapping function to each element. Each mapped stream is closed after its contentshave been placed into this stream. (If a mapped stream is nullan empty stream is used, instead.) 返回一个流，由将提供的映射函数应用到每个元素所产生的映射流的内容替换此流中的每个元素的结果组成。每个映射的流在其内容被放入该流后将被关闭。(如果映射流为null，则使用空流。) The flatMap() operation has the effect of applying a one-to-manytransformation to the elements of the stream, and then flattening theresulting elements into a new stream. flatMap()操作的效果是对流的元素应用一对多的转换，然后将产生的元素平铺成一个新的流。 示例： 123456List&lt;String&gt; nameList = Stream.of("ZhangSan", "Tom").collect(Collectors.toList());List&lt;String&gt; list = nameList.stream() .map(n -&gt; n.split("")) .flatMap(e -&gt; Arrays.stream(e)) .collect(Collectors.toList());System.out.println(list.toString()); 输出： 1[Z, h, a, n, g, S, a, n, T, o, m] 7.List集合转Map List&lt;User&gt; 集合，设置 User.name 作为Map的key，User对象作为value，转换为Map集合 12Map&lt;String, User&gt; userMap = userList.stream() .collect(Collectors.toMap(User::getName, Function.identity())); 8.针对List列表，按照指定元素分组，生成新的Map集合 例如对下面List列表的数据做分组 123new User(&quot;zhangsan&quot;, 12, &quot;Guangzhou&quot;);new User(&quot;lisi&quot;, 13, &quot;Shenzhen&quot;);new User(&quot;tom&quot;, 12, &quot;Beijing&quot;); 分组操作 1Map&lt;Integer, List&lt;User&gt;&gt; map = userList.stream().collect(Collectors.groupingBy(User::getAge)); 分组结果 123456789&#123; 12=[ User [name=zhangsan, age=12, address=Guangzhou], User [name=tom, age=12, address=Beijing] ], 13=[ User [name=lisi, age=13, address=Shenzhen] ]&#125; 9.根据指定元素去重 第1部分已经介绍了distinct去重方法，但是这种方法只能针对整个对象去重，如果想只根据对象中的某几个元素去重，可以通过下面的方法 1234User(name=zhangsan, sex=man, ages=1)User(name=zhangsan, sex=man, ages=2)User(name=lisi, sex=woman, ages=3)User(name=wanger, sex=man, ages=4) 如果只根据name和sex去重 123456userList.stream().collect( Collectors.collectingAndThen( Collectors.toCollection(() -&gt; new TreeSet&lt;&gt;( Comparator.comparing(p -&gt; p.getName()+";"+p.getSex())) ) , ArrayList::new)).forEach(System.out::println); 结果： 123User(name=lisi, sex=woman, ages=3)User(name=wanger, sex=man, ages=4)User(name=zhangsan, sex=man, ages=1)]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处理SpringCloudConfig客户端启动无法读取到配置参数]]></title>
    <url>%2F2023%2F02%2F01%2F%E5%A4%84%E7%90%86SpringCloudConfig%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%90%AF%E5%8A%A8%E6%97%A0%E6%B3%95%E8%AF%BB%E5%8F%96%E5%88%B0%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[自己部署了一个Spring Cloud微服务项目，实践Spring Cloud Config分布式配置组件，按照Spring Cloud Config 资料Config：Spring Cloud分布式配置组件 先后创建了Eureka注册中心服务、 Spring Cloud Config Server服务、 Spring Cloud Config Client客户端，在最后启动 Spring Client Config Client 客户端时，客户端始终无法访问 Config Server服务，读取上传在Gitee上的配置文件的内容。 在Baidu、 Google搜索了大量资料，问题是最终解决了，但是这其中的原因，还需要继续探讨。 Eureka注册中心和Spring Cloud Config Server的配置内容就不多讲，可参考Eureka：Spring Cloud服务注册与发现组件 和Config：Spring Cloud分布式配置组件，启动了 Config Server 服务，并用浏览器访问，上传在Gitee上的参数文件的内容是可以正常获取到的 我们重点说一下Spring Cloud Config Client的配置，yml文件配置如下： 1234567891011121314151617server: port: 3355 #端口号spring: application: name: spring-cloud-config-client #服务名 cloud: config: label: master #分支名称 name: application #配置文件名称，application-dev.yml 中的 config profile: dev #环境名 application-dev.yml 中的 dev #这里不要忘记添加 http:// 否则无法读取 uri: http://localhost:3344 #Spring Cloud Config 服务端（配置中心）地址eureka: client: #将客户端注册到 eureka 服务列表内 service-url: defaultZone: http://localhost:9900/eureka 新增Controller类，用于测试配置文件内容的读取 1234567891011121314151617181920@RestController@RequestMapping(&quot;/config/client&quot;)public class ConfigClientController &#123; @Value(&quot;$&#123;server.port&#125;&quot;) private String serverPort; @Value(&quot;$&#123;config.info&#125;&quot;) private String configInfo; @Value(&quot;$&#123;config.version&#125;&quot;) private String configVersion; @GetMapping(value = &quot;/getConfig&quot;) public String getConfig() &#123; return &quot;info：&quot; + configInfo + &quot;&lt;br/&gt;version：&quot; + configVersion + &quot;&lt;br/&gt;port：&quot; + serverPort; &#125;&#125; Spring Cloud Config Client客户端在启动的时候控制台报错： 123456789101112131415161718Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder &apos;config.info&apos; in value &quot;$&#123;config.info&#125;&quot; at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:180) ~[spring-core-5.3.23.jar:5.3.23] at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126) ~[spring-core-5.3.23.jar:5.3.23] at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:239) ~[spring-core-5.3.23.jar:5.3.23] at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:210) ~[spring-core-5.3.23.jar:5.3.23] at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:191) ~[spring-context-5.3.23.jar:5.3.23] at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:936) ~[spring-beans-5.3.23.jar:5.3.23] at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1332) ~[spring-beans-5.3.23.jar:5.3.23] at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311) ~[spring-beans-5.3.23.jar:5.3.23] at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.resolveFieldValue(AutowiredAnnotationBeanPostProcessor.java:656) ~[spring-beans-5.3.23.jar:5.3.23] at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:639) ~[spring-beans-5.3.23.jar:5.3.23] at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:119) ~[spring-beans-5.3.23.jar:5.3.23] at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:399) ~[spring-beans-5.3.23.jar:5.3.23] ... 16 common frames omittedDisconnected from the target VM, address: &apos;127.0.0.1:54601&apos;, transport: &apos;socket&apos;Process finished with exit code 1 遂检查配置文件，对照资料教程看是不是自己写错了。在检查 Config Client 模块的配置文件时发现，资料上创建的配置文件名称是 bootstrap.yml 而非 application.yml 遂把配置文件名改为 bootstrap.yml， 重新启动，发现没有报之前的错误了。但是服务也没有正常运行起来，而是直接停止了，控制台输出： 123456789101112131415161718192023-02-01 20:42:52.002 INFO 15096 --- [ main] com.netflix.discovery.DiscoveryClient : Starting heartbeat executor: renew interval is: 302023-02-01 20:42:52.005 INFO 15096 --- [ main] c.n.discovery.InstanceInfoReplicator : InstanceInfoReplicator onDemand update allowed rate per min is 42023-02-01 20:42:52.012 INFO 15096 --- [ main] com.netflix.discovery.DiscoveryClient : Discovery Client initialized at timestamp 1675255372011 with initial instances count: 22023-02-01 20:42:52.015 INFO 15096 --- [ main] o.s.c.n.e.s.EurekaServiceRegistry : Registering application SPRING-CLOUD-CONFIG-CLIENT with eureka with status UP2023-02-01 20:42:52.016 INFO 15096 --- [ main] com.netflix.discovery.DiscoveryClient : Saw local status change event StatusChangeEvent [timestamp=1675255372016, current=UP, previous=STARTING]2023-02-01 20:42:52.018 INFO 15096 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_SPRING-CLOUD-CONFIG-CLIENT/DESKTOP-A5PHDVG:spring-cloud-config-client:3355: registering service...2023-02-01 20:42:52.033 INFO 15096 --- [ main] c.s.c.ConfigClientApplication : Started ConfigClientApplication in 10.224 seconds (JVM running for 10.92)2023-02-01 20:42:52.040 INFO 15096 --- [ionShutdownHook] o.s.c.n.e.s.EurekaServiceRegistry : Unregistering application SPRING-CLOUD-CONFIG-CLIENT with eureka with status DOWN2023-02-01 20:42:52.040 INFO 15096 --- [ionShutdownHook] com.netflix.discovery.DiscoveryClient : Saw local status change event StatusChangeEvent [timestamp=1675255372040, current=DOWN, previous=UP]2023-02-01 20:42:52.042 INFO 15096 --- [ionShutdownHook] com.netflix.discovery.DiscoveryClient : Shutting down DiscoveryClient ...2023-02-01 20:42:52.076 INFO 15096 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_SPRING-CLOUD-CONFIG-CLIENT/DESKTOP-A5PHDVG:spring-cloud-config-client:3355 - registration status: 2042023-02-01 20:42:52.077 INFO 15096 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_SPRING-CLOUD-CONFIG-CLIENT/DESKTOP-A5PHDVG:spring-cloud-config-client:3355: registering service...2023-02-01 20:42:52.080 INFO 15096 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_SPRING-CLOUD-CONFIG-CLIENT/DESKTOP-A5PHDVG:spring-cloud-config-client:3355 - registration status: 2042023-02-01 20:42:52.081 INFO 15096 --- [ionShutdownHook] com.netflix.discovery.DiscoveryClient : Unregistering ...2023-02-01 20:42:52.085 INFO 15096 --- [ionShutdownHook] com.netflix.discovery.DiscoveryClient : DiscoveryClient_SPRING-CLOUD-CONFIG-CLIENT/DESKTOP-A5PHDVG:spring-cloud-config-client:3355 - deregister status: 2002023-02-01 20:42:52.091 INFO 15096 --- [ionShutdownHook] com.netflix.discovery.DiscoveryClient : Completed shut down of DiscoveryClientDisconnected from the target VM, address: &apos;127.0.0.1:58233&apos;, transport: &apos;socket&apos;Process finished with exit code 0 其中有一句 Registering application SPRING-CLOUD-CONFIG-CLIENT with eureka with status UP， 查询资料，在这篇文章SpringCloud中Client向Eureka注册中心注册服务成功后不久就Unregistering（Unregistering application 服务名 with eureka with）中有提出解决办法 虽然 Config Client 子模块依赖的父模块中，pom文件已经引入了spring-boot-web 依赖，但是依旧要在 Config Client 子模块的pom文件上加上 spring-boot-web 依赖 12345&gt; &lt;dependency&gt;&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&gt; &lt;/dependency&gt;&gt; 再次启动，服务启动成功 12345678923-02-01 20:51:37.092 INFO 16848 --- [ main] c.n.discovery.InstanceInfoReplicator : InstanceInfoReplicator onDemand update allowed rate per min is 42023-02-01 20:51:37.098 INFO 16848 --- [ main] com.netflix.discovery.DiscoveryClient : Discovery Client initialized at timestamp 1675255897097 with initial instances count: 12023-02-01 20:51:37.100 INFO 16848 --- [ main] o.s.c.n.e.s.EurekaServiceRegistry : Registering application SPRING-CLOUD-CONFIG-CLIENT with eureka with status UP2023-02-01 20:51:37.100 INFO 16848 --- [ main] com.netflix.discovery.DiscoveryClient : Saw local status change event StatusChangeEvent [timestamp=1675255897100, current=UP, previous=STARTING]2023-02-01 20:51:37.103 INFO 16848 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_SPRING-CLOUD-CONFIG-CLIENT/DESKTOP-A5PHDVG:spring-cloud-config-client:3355: registering service...2023-02-01 20:51:37.138 INFO 16848 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 3355 (http) with context path &apos;&apos;2023-02-01 20:51:37.139 INFO 16848 --- [ main] .s.c.n.e.s.EurekaAutoServiceRegistration : Updating port to 33552023-02-01 20:51:37.164 INFO 16848 --- [nfoReplicator-0] com.netflix.discovery.DiscoveryClient : DiscoveryClient_SPRING-CLOUD-CONFIG-CLIENT/DESKTOP-A5PHDVG:spring-cloud-config-client:3355 - registration status: 2042023-02-01 20:51:38.241 INFO 16848 --- [ main] c.s.c.ConfigClientApplication : Started ConfigClientApplication in 12.417 seconds (JVM running for 13.157) Eureka注册中心 浏览器调用接口 那么，为什么Spring Cloud Config Client 的配置文件为什么要用 bootstrap.yml， 而不是 application ？ 这里有一篇文章有说明SpringCloud Config - client连接server的设置写在application.yml, 导致属性无法解析 Bootstrap.yml (bootstrap.properties) 是在application.yml (application.properties)之前加载的。它通常用于“使用SpringCloud Config Server时，应在bootstrap.yml中指定spring.application.name和spring.cloud.config.server.git.uri”以及一些加密/解密信息。 Spring Cloud会创建一个Bootstrap Context（由bootstrap.yml加载），作为Spring应用的Application Context（由application.yml加载）的父上下文。初始化的时候，Bootstrap Context负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的Environment。Bootstrap属性有高优先级，默认情况下，它们不会被本地配置覆盖。 例如，当使用SpringCloud Config时，通常从服务器加载“真正的”配置数据。为了获取URL（和其他连接配置，如密码等），您需要一个较早的或“bootstrap”配置。因此，您将配置服务器属性放在bootstrap.yml中，该属性用于加载实际配置数据（通常覆盖application.yml [如果存在]中的内容）。 补充： 在刚开始启动Spring Cloud Config Client 时，控制台提示： 1234567891011121314Description:No spring.config.import property has been definedAction:Add a spring.config.import=configserver: property to your configuration. If configuration is not required add spring.config.import=optional:configserver: instead. To disable this check, set spring.cloud.config.enabled=false or spring.cloud.config.import-check.enabled=false.Disconnected from the target VM, address: &apos;127.0.0.1:58966&apos;, transport: &apos;socket&apos;Process finished with exit code 1 stackoverflow上有篇文章No spring.config.import property has been defined中给出解决办法，在pom文件中加上依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bootstrap&lt;/artifactId&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Cloud Config</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arrays.asList()生成的对象不能执行add()的操作]]></title>
    <url>%2F2023%2F01%2F12%2FArrays-asList-%E7%94%9F%E6%88%90%E7%9A%84%E5%AF%B9%E8%B1%A1%E4%B8%8D%E8%83%BD%E6%89%A7%E8%A1%8Cadd-%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[之前项目上，有使用 Arrays.asList() 创建一个List集合，并在后续的操作中使用之前创建的List集合继续 add() 添加元素。 12List&lt;String&gt; list = Arrays.asList("one", "two", "three");list.add("five"); 运行项目却在 list.add("five"); 处报错： 1234Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException at java.util.AbstractList.add(AbstractList.java:148) at java.util.AbstractList.add(AbstractList.java:108) ... 网上搜索资料得知， Arrays.asList() 生成的 ArrayList 对象是 Arrays 自己的内部类对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/** * Returns a fixed-size list backed by the specified array. (Changes to * the returned list "write through" to the array.) This method acts * as bridge between array-based and collection-based APIs, in * combination with &#123;@link Collection#toArray&#125;. The returned list is * serializable and implements &#123;@link RandomAccess&#125;. * * &lt;p&gt;This method also provides a convenient way to create a fixed-size * list initialized to contain several elements: * &lt;pre&gt; * List&amp;lt;String&amp;gt; stooges = Arrays.asList("Larry", "Moe", "Curly"); * &lt;/pre&gt; * * @param &lt;T&gt; the class of the objects in the array * @param a the array by which the list will be backed * @return a list view of the specified array */@SafeVarargs@SuppressWarnings("varargs")public static &lt;T&gt; List&lt;T&gt; asList(T... a) &#123; return new ArrayList&lt;&gt;(a);&#125;/** * @serial include */private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, java.io.Serializable&#123; private static final long serialVersionUID = -2764017481108945198L; private final E[] a; ArrayList(E[] array) &#123; a = Objects.requireNonNull(array); &#125; @Override public int size() &#123; return a.length; &#125; @Override public Object[] toArray() &#123; return a.clone(); &#125; @Override @SuppressWarnings("unchecked") public &lt;T&gt; T[] toArray(T[] a) &#123; int size = size(); if (a.length &lt; size) return Arrays.copyOf(this.a, size, (Class&lt;? extends T[]&gt;) a.getClass()); System.arraycopy(this.a, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a; &#125; @Override public E get(int index) &#123; return a[index]; &#125; @Override public E set(int index, E element) &#123; E oldValue = a[index]; a[index] = element; return oldValue; &#125; @Override public int indexOf(Object o) &#123; E[] a = this.a; if (o == null) &#123; for (int i = 0; i &lt; a.length; i++) if (a[i] == null) return i; &#125; else &#123; for (int i = 0; i &lt; a.length; i++) if (o.equals(a[i])) return i; &#125; return -1; &#125; @Override public boolean contains(Object o) &#123; return indexOf(o) != -1; &#125; @Override public Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator(a, Spliterator.ORDERED); &#125; @Override public void forEach(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); for (E e : a) &#123; action.accept(e); &#125; &#125; @Override public void replaceAll(UnaryOperator&lt;E&gt; operator) &#123; Objects.requireNonNull(operator); E[] a = this.a; for (int i = 0; i &lt; a.length; i++) &#123; a[i] = operator.apply(a[i]); &#125; &#125; @Override public void sort(Comparator&lt;? super E&gt; c) &#123; Arrays.sort(a, c); &#125;&#125; ArrayList 继承自 AbstractList，而 AbstractList 的 add() 方法抛出 UnsupportedOperationException 异常。 123456789101112131415/** * &#123;@inheritDoc&#125; * * &lt;p&gt;This implementation always throws an * &#123;@code UnsupportedOperationException&#125;. * * @throws UnsupportedOperationException &#123;@inheritDoc&#125; * @throws ClassCastException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; * @throws IllegalArgumentException &#123;@inheritDoc&#125; * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public void add(int index, E element) &#123; throw new UnsupportedOperationException(); &#125; 当然，AbstractList 的 remove() 方法也是一样的 123456789101112/** * &#123;@inheritDoc&#125; * * &lt;p&gt;This implementation always throws an * &#123;@code UnsupportedOperationException&#125;. * * @throws UnsupportedOperationException &#123;@inheritDoc&#125; * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public E remove(int index) &#123; throw new UnsupportedOperationException(); &#125; 所以可以使用其他方式创建List集合对象 方法1： 1List&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList("a", "b")); 方法2： 1List&lt;String&gt; list = Stream.of("str1", "str2").collect(Collectors.toList());]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Arrays.asList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于《软件设计师教程第5版》UML构件图两类接口描述错误的问题]]></title>
    <url>%2F2022%2F11%2F03%2F%E5%85%B3%E4%BA%8E%E3%80%8A%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E5%B8%88%E6%95%99%E7%A8%8B%E7%AC%AC5%E7%89%88%E3%80%8BUML%E6%9E%84%E4%BB%B6%E5%9B%BE%E4%B8%A4%E7%B1%BB%E6%8E%A5%E5%8F%A3%E6%8F%8F%E8%BF%B0%E9%94%99%E8%AF%AF%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[如图所示，在备考软件设计师时，有遇到关于 构件图 的题目，官方给的 -（ 是供接口，O- 是需接口，就像官方教程上展示的这样。但是发现有其他人说 -（ 是需接口，O- 是供接口。这就很懵圈了，然后我搜索国内的资料，也都是说： -（ 是需接口，O- 是供接口 # UML设计图10-构件图 # 【UML简明教程】接口 - Tim的资源站 我也去外网上搜索了一下，搜索到的资料也都显示的是： -（ 是需接口，O- 是供接口 # The different types of interfaces in UML diagram # What is Component Diagram? 从互联网上多数资料来看，正确的应该是 -（ 是需接口，O- 是供接口]]></content>
      <categories>
        <category>升级之路</category>
      </categories>
      <tags>
        <tag>软件设计</tag>
        <tag>UML</tag>
        <tag>构件图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根据JetBrains Fleet文档创建maven项目失败的解决办法]]></title>
    <url>%2F2022%2F10%2F26%2F%E6%A0%B9%E6%8D%AEJetBrains-Fleet%E6%96%87%E6%A1%A3%E5%88%9B%E5%BB%BAmaven%E9%A1%B9%E7%9B%AE%E5%A4%B1%E8%B4%A5%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[打算尝试一下Fleet编辑器，根据JetBrains Fleet文档创建maven项目. 首选确认本地机器上已经安装并配置了maven 按照文档提示，通过CTRL+ALT+T 打开 terminal 窗口，在编辑器的下方 粘贴并执行 mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false 创建失败，提下如下 然后google之，在stackoverflow上发现了这篇文章 https://stackoverflow.com/questions/16348459/error-the-goal-you-specified-requires-a-project-to-execute-but-there-is-no-pom 简而言之：必须用引号括起所有参数 于是重新修改了mvn命令并重新执行 mvn archetype:generate "-DgroupId=com.mycompany.app" "-DartifactId=my-app" "-DarchetypeArtifactId=maven-archetype-quickstart" "-DarchetypeVersion=1.4" "-DinteractiveMode=false" 于是maven项目创建成功]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>根据JetBrains</tag>
        <tag>Fleet</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL根据出生日期计算当前年龄]]></title>
    <url>%2F2022%2F10%2F21%2FMySQL%E6%A0%B9%E6%8D%AE%E5%87%BA%E7%94%9F%E6%97%A5%E6%9C%9F%E8%AE%A1%E7%AE%97%E5%BD%93%E5%89%8D%E5%B9%B4%E9%BE%84%2F</url>
    <content type="text"><![CDATA[假如人员的出生日期为 1994-10-01，首先用 MySQL 的 now() 函数获取当前系统日期，然后利用DATE_FORMAT() 函数计算出当前年龄。 注意：DATE_FORMAT() 方法后面要加0 1select DATE_FORMAT(FROM_DAYS(DATEDIFF(now(), '1994-10-01')), '%Y')+0 as age 实践一下，当前系统时间为 2022-08-29 10:50:54 DATEDIFF() 函数返回两个日期之间的天数。 FROM_DAYS() 函数：给定一个天数N，并返回一个日期值。 DATE_FORMAT() 函数用于以不同的格式显示日期/时间数据。 使用FROM_DAYS()谨慎旧日期，它不打算使用与之前的公历(1582年)的到来值。 参考资料： https://www.tutorialspoint.com/calculate-age-based-on-date-of-birth-in-mysql]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处理MySQL使用concat()函数时遇到null值的问题]]></title>
    <url>%2F2022%2F10%2F21%2F%E5%A4%84%E7%90%86MySQL%E4%BD%BF%E7%94%A8concat-%E5%87%BD%E6%95%B0%E6%97%B6%E9%81%87%E5%88%B0null%E5%80%BC%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题描述 使用CONCAT()拼接结果是，当CONCAT()函数中的一个参数为null，那么不管其他字符串是否有值，最后返回的拼接结果总是null，如下所示： 123456SELECT name, address, nationality, CONCAT('my name is ', name, ', to live in ', address, ', and i am from ', nationality) as strFROM `user2` MySQL 官方文档有句话 解决办法 使用 COALESCE() 函数转换null值 123456SELECT name, address, nationality, CONCAT('my name is ', COALESCE(name, ''), ', to live in ', COALESCE(address, ''), ', and i am from ', COALESCE(nationality, '')) as strFROM `user2` 使用IFNULL() 函数转换null值 123456SELECT name, address, nationality, CONCAT('my name is ', ifnull(name, ''), ', to live in ', ifnull(address, ''), ', and i am from ', ifnull(nationality, '')) as strFROM `user2` 尝试使用CONCAT_WS() 函数拼接字符串 123456SELECT name, address, nationality, CONCAT_WS(',',name,address,nationality) as strFROM `user2` 参考资料： https://stackoverflow.com/questions/15741314/mysql-concat-returns-null-if-any-field-contain-null 12.8 String Functions and Operators 12.5 Flow Control Functions]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[货币的发行]]></title>
    <url>%2F2022%2F07%2F25%2F%E8%B4%A7%E5%B8%81%E7%9A%84%E5%8F%91%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[中国共产党的货币实践有一个特点，就是货币本位十分灵活（当然也有革命条件艰苦缺乏贵金属的因素），从革命战争年代到改革开放以前长期实行的是一种“物资本位”。下面以长征时期的货币发行实践举例，题主大概可以见微知著，了解“物资本位”货币的发行、流通与回收 1935年1月7日，红军攻占遵义城。遵义是贵州省内仅次于贵阳的第二大城市，一向为黔北中心，各种土产集散地，汉、苗等族商旅云集之所，市场十分繁华，是红军长征以来所经过的第一座繁华的中等城市。 红军大部队进城时，群众已发动起来，“各种商店都已开门营业，百货日用品、食品到处都是，街上行人也很多，有赶街市的，有看热闹的，有做宣传的，还有高声叫卖麻糖的”。久困大山、长期作战的红军指战员，得到这么一个难得的休整机会，来到这么一个有大量东西可买的地方，也急需用经费购买生活、医疗等用品，以利今后的行军打仗。 但由于红军指战员随身携带的经费一部分或全部是苏维埃国家银行在中央苏区发行的苏维埃纸币，它们在白区无法流通，得不到当地百姓的认可。另一方面，当年遵义作为国民党政府的一个行署，管辖十来个县，自是军阀、官僚、地主、奸商相互勾结的地方，贵州军阀王家烈便在这里称雄一方。他在遵义开有烟馆、盐行，囤积了价值几十万元的食盐和大量烟土。当地缺盐，他一手操纵市场，以高价出售。其时猪肉才两角钱一斤，盐巴倒要四角钱一斤。许多老百姓因为买不起盐而患大脖子病。 苏维埃国家银行对上述情况进行了充分的估量，认为这种一面到处是商机、商业活动，一面群众严重缺盐而军阀囤积居奇的局面，有利于发行苏维埃钞票，主导市场，满足群众需要。他们把没收来的大量食盐和其他稀缺物资以低价卖给群众，规定只收苏维埃钞票，不收其他货币。而且一元苏维埃钞票可到兑换点换取一元二角现洋，或两元国民党的钞票。由于国家银行有充足的现金(银圆、金条等)和物资保证，发行的苏维埃钞票完全可以兑现，遵义城及附近驻有红军部队的城镇的群众和商家都乐于接纳、使用苏维埃钞票，红军指战员得以用手中的苏维埃币购买必要的物资。苏维埃钞票信誉盛极一时，市面也空前繁荣。为此，国家银行的工作人员在遵义度过了最为忙碌的10天。 由于无法在遵义建立稳固的根据地，红军决定撤离遵义。在红军撤离遵义前，为避免百姓蒙受损失或招来报复，苏维埃国家银行连夜组织回笼苏维埃币。这样，短短的十几天里，苏维埃国家银行完成了苏维埃币的发行、流通与回收，创造了一个奇迹。 国家银行在遵义发行、回笼苏维埃钞票的做法，既活跃了市场，保障了红军的供给，又维护了苏维埃钞票的信誉，维护了群众的利益，因而在当地留下了很好的影响，树立了共产党和红军的良好形象。对于当年少数还流散在群众手中的苏维埃钞票，1955年国家发行新的人民币时，还以1∶1的比价兑回了。 -- 以上来自 知乎 方亮 关于货币发行的问题，可能大多数人是不清楚的，但是不了解货币发行的原理，对于各种经济学现象就不能真正的理解。关于中国目前货币是如何发行的，这样发行的道理又是什么，本人做了以下研究和思考。 首先国内货币的发行，离不开中国人民银行（简称央行，是中华人民共和国的中央银行）。本人理解央行和现代企业运行的基本规则是相似的。企业的主要职责是赚取利润，而央行的主要职责就是通过控制市场中货币的数量，来实现物价稳定（即通货膨胀指标）、经济发展、充分就业、国际收支平衡（汇率指标）等目标。央行对外提供借款，需要设定利率和借款期限，到期收回借款，央行可以对外支付对价购买外汇、债券等资产，这些都是和企业的运行规则是相似的。 央行控制市场中货币数量的方法主要包括向商业银行购买外汇和提供贷款。其中提供贷款又可细分为MLF、PSL等公开市场操作投放和再贷款、再贴现投放。 首先要说明的央行向商业银行购买外汇。目前国际贸易中主要是以美元结算，因为美国的超级大国的国际地位，让美元成为类似于黄金的硬通货。正是因为美元作为国际结算的主要货币，所以国家需要储备美元资产。国内的对外贸易公司通过向国外出售商品赚取到了美元，将一部分需要兑换成人民币的美元出售给中国银行（国内一般都是出售给中国银行），央行再向商业银行购买这部分外汇。央行购买的外汇向国外换为其他资产，如下图所示。 央行购买外汇支付的是债券（即人民币），这里债券相当于借条，只不过因为央行的地位，这张借条有了在市场流通的价值，可以在市场中购买资产，所以也就不需要用这张借条向央行要求还资产了。（在这里要说明的是，央行对外支付的货币可以是纸质也可以是账户里的余额，两者是等价的。在现实生活中，纸质货币能满足大家的一些使用需求，所以央行需要投放和更换市场中的纸质货币，满足大家对纸质货币的使用需求。但市场中的货币，现金（M0)只占其中一小部分，货币大部分以银行存款的余额和其他形式存在，这里不做过多解释。）央行和商业银行的资产负债表如下图所示。 央行从商业银行手中买入资产，通过增加商业银行在央行账户的存款准备金为支付手段。商业银行可以在超出央行规定的存款准备金比例之外的资金支取，用于银行的对外贷款等业务，从而增加了货币在市场中的数量。 央行在向商业银行提供贷款中，央行增加债权资产和商业银行存款负债，商业银行增加负债和央行存款。央行向商业银行提供贷款同样增加了市场中货币的数量。 增加的货币数量，通过货币乘数，转化为真正的货币增量。（具体原理：假设甲有1000元的现金，存在商业银行A活期存款帐户。银行按照中央银行的要求，把其中10%（100元）交纳法定准备金，存入中央银行。剩下的900元可以全部贷出去，贷给乙。乙把这些钱再存入乙在B银行的帐户，B银行再交给中国人民银行90元，剩下的再贷出去。周而复始，理论上可以产生10000（1000/10%）元的活期存款。那么这个货币乘数就是10。10000元为真正增加的货币数量） 以上是央行控制市场中货币数量的方法，央行会根据市场是否缺少流动性，来调整其以上方法的使用力度，达到其想要实施的货币政策。 市场中货币数量的增加，在总商品数量不变的情况下，会引起商品价格的上升，货币总是追逐相对有些的商品引起价格的上升。因此当货币发行的速度，大于GDP的增长速度，那么就会带来通货膨胀。在通货膨胀的过程中，谁会受益呢？应该是能从价格上升中得到益处的人。通过努力，在这个过程中拥有较多具有保值功能的资产的人会受益。还有谁会受益呢，能够利用流动性宽裕的人，通过其良好的运作，可以在其中获益。 -- 以上来自 知乎 大峰哥]]></content>
      <categories>
        <category>兴趣使然</category>
      </categories>
      <tags>
        <tag>货币</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL事务]]></title>
    <url>%2F2020%2F04%2F26%2FMySQL%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[MySQL数据库事务是一组原子性的SQL单元，事务内的语句要么全部执行成功，要么全部执行失败。 事务的四个特性 原子性（Atomicity） 事务必须是原子工作单元，事务中的操作要么全部执行成功，要么全部执行失败，不能只执行部分操作。 一致性 事务开始之前，数据库处于一致性的状态；事务结束后，数据库必须仍处于一致性状态。数据库一致性的定义是由用户负责的。例如，在银行转账中，用户可以定义转账前后两个账户金额之和保持不变。 隔离性 通常来说一个事务在修改时但还未提交，对其他事务是不可见的。 持久性 -一旦事务提交，则其所做的修改就会永久保存在数据库中。此时即使系统崩溃，修改的数据也不会丢失。 事务的脏读、不可重复读、幻读 脏读：事务 A 读取了事务 B 更新后的数据，但是事务 B 没有提交，然后事务 B 执行回滚操作，那么事务 A 读到的数据就是脏数据。 不可重复读：事务 A 进行多次读取操作，事务 B 在事务 A 多次读取的过程中执行更新操作并提交，提交后事务 A 读到的数据不一致。 幻读：事务 A 将数据库中所有学生的成绩由 A -&gt; B，此时事务 B 手动插入了一条成绩为 A 的记录，在事务 A 更改完毕后，发现还有一条记录没有修改，那么这种情况就叫做出现了幻读。 事务的隔离级别 读未提交(read uncommitted)、读已提交(read committed)、可重复读(repetable read) 和 串行化(serializable)。 读未提交：读未提交指的是一个事务在提交之前，它所做的修改就能够被其他事务所看到。 读已提交：读已提交指的是一个事务在提交之后，它所做的变更才能够让其他事务看到。 可重复读：可重复读指的是一个事务在执行的过程中，看到的数据是和启动时看到的数据是一致的。未提交的变更对其他事务不可见。 串行化：顾名思义是对于同一行记录，写会加写锁，读会加读锁。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 隔离级别由低到高是：读未提交 &lt; 读已提交 &lt; 可重复读 &lt; 串行化 MySQL 常见存储引擎的区别 通过SQL语句 SHOW ENGINES查看存储引擎 可以看到，InnoDB 是 MySQL 默认支持的存储引擎，支持事务、行级锁定和外键。 MyISAM 存储引擎的特点 在 5.1 版本之前，MyISAM 是 MySQL 的默认存储引擎，MyISAM 并发性比较差，使用的场景比较少，主要特点是 不支持事务操作，ACID 的特性也就不存在了，这一设计是为了性能和效率考虑的。 不支持外键操作，如果强行增加外键，MySQL 不会报错，只不过外键不起作用。 MyISAM 默认的锁粒度是表级锁，所以并发性能比较差，加锁比较快，锁冲突比较少，不太容易发生死锁的情况。 MyISAM 会在磁盘上存储三个文件，文件名和表名相同，扩展名分别是 .frm(存储表定义)、.MYD(MYData,存储数据)、MYI(MyIndex,存储索引)。这里需要特别注意的是 MyISAM 只缓存索引文件，并不缓存数据文件。 MyISAM 支持的索引类型有 全局索引(Full-Text)、B-Tree 索引、R-Tree 索引 Full-Text 索引：它的出现是为了解决针对文本的模糊查询效率较低的问题。 B-Tree 索引：所有的索引节点都按照平衡树的数据结构来存储，所有的索引数据节点都在叶节点 R-Tree索引：它的存储方式和 B-Tree 索引有一些区别，主要设计用于存储空间和多维数据的字段做索引,目前的 MySQL 版本仅支持 geometry 类型的字段作索引，相对于 BTREE，RTREE 的优势在于范围查找。 数据库所在主机如果宕机，MyISAM 的数据文件容易损坏，而且难以恢复。 增删改查性能方面：SELECT 性能较高，适用于查询较多的情况 InnoDB 存储引擎的特点 自从 MySQL 5.1 之后，默认的存储引擎变成了 InnoDB 存储引擎，相对于 MyISAM，InnoDB 存储引擎有了较大的改变，它的主要特点是 支持事务操作，具有事务 ACID 隔离特性，默认的隔离级别是可重复读(repetable-read)、通过MVCC（并发版本控制）来实现的。能够解决脏读和不可重复读的问题。 InnoDB 支持外键操作。 InnoDB 默认的锁粒度行级锁，并发性能比较好，会发生死锁的情况。 和 MyISAM 一样的是，InnoDB 存储引擎也有 .frm文件存储表结构 定义，但是不同的是，InnoDB 的表数据与索引数据是存储在一起的，都位于 B+ 数的叶子节点上，而 MyISAM 的表数据和索引数据是分开的。 InnoDB 有安全的日志文件，这个日志文件用于恢复因数据库崩溃或其他情况导致的数据丢失问题，保证数据的一致性。 InnoDB 和 MyISAM 支持的索引类型相同，但具体实现因为文件结构的不同有很大差异。 增删改查性能方面，如果执行大量的增删改操作，推荐使用 InnoDB 存储引擎，它在删除操作时是对行删除，不会重建表。 MyISAM 和 InnoDB 存储引擎的对比 锁粒度方面：由于锁粒度不同，InnoDB 比 MyISAM 支持更高的并发；InnoDB 的锁粒度为行锁、MyISAM 的锁粒度为表锁、行锁需要对每一行进行加锁，所以锁的开销更大，但是能解决脏读和不可重复读的问题，相对来说也更容易发生死锁 可恢复性上：由于 InnoDB 是有事务日志的，所以在产生由于数据库崩溃等条件后，可以根据日志文件进行恢复。而 MyISAM 则没有事务日志。 查询性能上：MyISAM 要优于 InnoDB，因为 InnoDB 在查询过程中，是需要维护数据缓存，而且查询过程是先定位到行所在的数据块，然后在从数据块中定位到要查找的行；而 MyISAM 可以直接定位到数据所在的内存地址，可以直接找到数据。 表结构文件上：MyISAM 的表结构文件包括：.frm(表结构定义),.MYI(索引),.MYD(数据)；而 InnoDB 的表数据文件为:.ibd和.frm(表结构定义)； 文章来自Java建设者]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程]]></title>
    <url>%2F2020%2F04%2F14%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Java 给多线程编程提供了内置的支持。 一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 多线程是多任务的一种特别的形式，但多线程使用了更小的资源开销。 这里定义和线程相关的另一个术语 - 进程：一个进程包括由操作系统分配的内存空间，包含一个或多个线程。一个线程不能独立的存在，它必须是进程的一部分。一个进程一直运行，直到所有的非守护线程都结束运行后才能结束。 多线程能满足程序员编写高效率的程序来达到充分利用 CPU 的目的。 线程和进程的区别 线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存来存储本地数据。 创建线程有哪几种方式？分别有什么优缺点？ 创建线程 继承Thread类，重写run()方法； 实现Runnable接口，实现run()方法，并将该实现类作为参数传入Thread构造器。 实现Callable接口，重写call()方法，并包装成FutureTask对象，再作为参数传入Thread构造器。 继承Thread类优缺点 优点是编码简单，缺点是不能再继承其他的类，功能单一。 实现Runnable接口优缺点 优点是可以继承其他类，避免单继承的局限性；适合多个相同程序代码的线程共享一个资源，实现解耦操作，代码和线程独立。缺点是实现相对复杂。 实现Callable接口优缺点 优点是相比方式二可以获取返回值，缺点是实现复杂。 线程的状态 new（新建）：线程刚刚被创建，但是并未启动，还没有调用start方法。 Runnable（可运行）：线程可以在Java虚拟机中执行的状态，但是这个“执行”，不一定是真的运行，也可能是在等待CPU资源。所以在网上，有人把这个状态区分为READY和RUNNING两个，一个表示的start了，资源一到位随时可以执行，另一个表示真正的执行中。 Blocked（锁阻塞）：当一个线程试图获取一个锁对象，而该对象被其他的线程持有，则该线程进入Blocked状态；比较经典的就是synchronized关键字，这个关键字修饰的代码块或者方法，均需要获取到对应的锁，在未获取之前，其线程的状态就一直为BLOCKED，当该线程持有锁时，该线程将变成Runnable状态。如果线程长时间处于这种状态下，我们就要当心，看看是否出现死锁的问题了。 Waiting（无限等待）：一个线程在等待另一个线程执行动作是，该线程进入Waiting状态。进入这个状态后是不能自动唤醒的，必须等待另一个线程调用notify或者notifyAll方法才能够唤醒。 Timed Waiting（计时等待）：同waiting状态，有几个方法有超时参数，调用他们将进入Timed Waiting状态。如： Object.wait() Thread.join() Thread.sleep LockSupport.park() 这一状态将一直保持到超时期满或者收到唤醒通知。 THRMINATED（死亡状态）：因为run方法正常退出而死亡，或者因为没有捕获的异常终止了run方法而死亡。 volatile关键字 保证被修饰的变量对所有线程可见，在一个线程修改一个变量的值后，新值对其他线程是可以立即获取的。 禁止指令重排序，被修饰的变量不会被缓存在寄存器中或者对其他处理器不可见的地方，因此在读取volatile修饰的变量时总是会返回最新写入的值。 不会执行加锁操作，不会导致线程阻塞，主要适用于一个变量被多个线程共享，多个线程均可对这个变量执行赋值或读取的操作， volatile可以严格保证变量的单次读写操作的原子性，但不能保证像i++ 这种操作的原子性，因为i++ 在本质上是读、写两次操作。 synchronized关键字 用于为Java对象、方法、代码块提供线程安全的操作，属于排它的悲观锁，也属于可重入锁。 被synchronzied修饰的方法和代码块在同一时刻只能有一个线程访问，其他线程只能等待当前线程释放资源后才能访问。 Java中的每个对象都有一个monitor监视器对象，加锁就是在竞争monitor，对代码块加锁是通过在前后分别加上monitorenter和monitorexit指令实现的，对方是否加锁是通过一个标记位来判断的。 线程池是什么？为什么需要线程池？ 在生产中为每一个任务创建一个线程存在一些缺陷，如果无限制地大量创建线程会消耗很多资源，影响系统稳定性和性能，产生内存溢出等问题。 线程池是管理一组同构工作线程的资源池，线程池与工作队列密切相关，工作队列中保存了所有需要等待执行的任务。工作线程的任务很简单，从工作队列获取任务，执行任务，返回线程池并等待下一次任务。 线程池通过重用现有的线程，可以在处理多个请求时分摊线程在创建和撤销过程中的开销，另一个好处是当请求到达时工作线程通常已经存在，不会出现等待线程而延迟的任务的执行，提高了响应性。通过调整线程池的大小，可以创建足够多的线程保持处理器处于忙碌状态，同时还可以防止线程过多导致内存资源耗尽。 创建线程池时，ThreadPoolExecutor构造器中都有哪些参数以及各自的含义 corePoolSize： 线程池核心大小，即在没有任务执行时线程池的大小，并且只有在工作队列满了的情况下才会创建超出这个数量的线程。 maximumPoolSize： 线程池最大大小，表示可同时活动的线程数量的上限。 keepAliveTime：存活时间，如果某个线程的空闲时间超过了存活时间，那么将被标记为可回收的，并且当线程池的当前大小超过基本大小时，这个线程将被终止。 unit： 存活时间的单位，可选的参数为TimeUnit枚举中的几个静态变量： NANOSECONDS、MICROSECONDS、MILLISECONDS、SECONDS。 workQueue： 线程池所使用的阻塞队列。 thread factory：线程池使用的创建线程工厂方法，可省略，将使用默认工厂。 handler：所用的拒绝执行处理策略，可省略，将使用默认拒绝执行策略。 创建线程池的方法有哪些？ 可以通过Executors的静态工厂方法创建线程池，内部通过重载ThreadExecutorPool不同的构造器创建线程池。 newFixedThreadPool，创建一个固定长度的线程池，每当提交一个任务就创建一个线程，直到达到线程池的最大数量，这时线程池的规模将不再变化(如果某个线程由于发生了未预期的异常而结束，那么线程池会补充一个新的线程)。将线程池的核心大小和最大大小都设置为参数中指定的值，创建的线程不会超时，使用LinkedBlockingQueue。 newCachedThreadPool，创建一个可缓存的线程池，如果线程池的当前规模超过了处理器需求，那么将回收空闲的线程，而当需求增加时，可以添加新的线程，线程池的规模不存在任何限制。将线程池的最大大小设置为Integer.MAX_VALUE，而将核心大小设置为0，并将超时设为1分钟，使用SynchronousQueue，这种方法创建出的线程池可被无限扩展，并当需求降低时自动收缩。 newSingleThreadExecutor，一个单线程的Executor，创建单个工作者线程来执行任务，如果这个线程异常结束，会创建另一个线程来代替。确保依照任务在队列中的顺序来串行执行。将核心线程和最大线程数都设置为1，使用LinkedBlockingQueue。 newScheduledThreadPool，创建一个固定长度的线程池，而且以延迟或定时的方式来执行任务，类似于Timer，使用DelayedWorkQueue。 线程池的工作原理 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。即使队列里面有任务，线程池也不会马上执行它们。 通过 execute(Runnable command)方法被添加到线程池，任务就是一个 Runnable类型的对象，任务的执行方法就是Runnable类型对象的run()方法。 如果workerCount 小于corePoolSize，那么创建并启动一个线程执行新提交的任务。如果workerCount大于等于corePoolSize，且线程池内的阻塞队列未满，那么将这个任务放入队列。如果workerCount大于等于corePoolSize，且阻塞队列已满，若满足workerCount小于maximumPoolSize,那么还是要创建并启动一个线程执行新提交的任务。若阻塞队列已满，并且workerCount大于等于maximumPoolSize，则根据 handler所指定的策略来处理此任务，默认的处理方式直接抛出异常。也就是处理任务的优先级为： 核心线程corePoolSize、任务队列workQueue、最大线程maximumPoolSize，如果三者都满了，使用handler处理被拒绝的任务。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程没有任务可执行，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于corePoolSize时，那么这个线程会被停用掉，所以线程池的所有任务完成后，它最终会收缩到corePoolSize的大小。 start和run方法的区别 start方法用于启动线程，真正实现了多线程，调用了start方法后，会在后台创建一个新的线程来执行，不需要等待run方法执行完毕就可以继续执行其他代码。调用start方法时，该线程处于就绪状态，并没有开始运行。 run方法也叫做线程体，包含了要执行的线程的逻辑代码，在调用run方法并没有创建新的线程，而是直接运行run方法中的代码。 References https://blog.csdn.net/qq_41112238/article/details/105074636 https://baijiahao.baidu.com/s?id=1655501582187466001&amp;wfr=spider&amp;for=pc https://www.jianshu.com/p/ec94ed32895f]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>线程</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试官邪魅一笑：MySQL千万级别大表，你要如何优化？]]></title>
    <url>%2F2020%2F04%2F09%2F%E9%9D%A2%E8%AF%95%E5%AE%98%E9%82%AA%E9%AD%85%E4%B8%80%E7%AC%91%EF%BC%9AMySQL%E5%8D%83%E4%B8%87%E7%BA%A7%E5%88%AB%E5%A4%A7%E8%A1%A8%EF%BC%8C%E4%BD%A0%E8%A6%81%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[当MySQL单表记录数过大时，增删改查性能都会急剧下降，可以参考以下步骤来优化： 单表优化 除非单表数据未来会一直不断上涨，否则不要一开始就考虑拆分，拆分会带来逻辑、部署、运维的各种复杂度，一般以整型值为主的表在千万级以下，字符串为主的表在五百万以下是没有太大问题的。而事实上很多时候MySQL单表的性能依然有不少优化空间，甚至能正常支撑千万级以上的数据量： 字段 尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED VARCHAR的长度只分配真正需要的空间 使用枚举或整数代替字符串类型 尽量使用TIMESTAMP而非DATETIME， 单表不要有太多字段，建议在20以内 避免使用NULL字段，很难查询优化且占用额外索引空间 用整型来存IP 索引 索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描 应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描 值分布很稀少的字段不适合建索引，例如"性别"这种只有两三个值的字段 字符字段只建前缀索引 字符字段最好不要做主键 不用外键，由程序保证约束 尽量不用UNIQUE，由程序保证约束 使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引 查询SQL 可通过开启慢查询日志来找出较慢的SQL 不做列运算：SELECT id WHERE age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边 sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库 不用`SELECT *`` OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内 不用函数和触发器，在应用程序实现 避免%xxx式查询 少用JOIN 使用同类型进行比较，比如用'123'和'123'比，123和123比 尽量避免在WHERE子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描 对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5 列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大 引擎 目前广泛使用的是MyISAM和InnoDB两种引擎： MyISAM MyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是： 不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁 不支持事务 不支持外键 不支持崩溃后的安全恢复 在表有读取查询的同时，支持往表中插入新纪录 支持BLOB和TEXT的前500个字符索引，支持全文索引 支持延迟更新索引，极大提升写入性能 对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用 InnoDB InnoDB在MySQL 5.5后成为默认索引，它的特点是： 支持行锁，采用MVCC来支持高并发 支持事务 支持外键 支持崩溃后的安全恢复 不支持全文索引 总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表 系统调优参数 可以使用下面几个工具来做基准测试： sysbench：一个模块化，跨平台以及多线程的性能测试工具 iibench-mysql：基于 Java 的 MySQL/Percona/MariaDB 索引进行插入性能测试工具 tpcc-mysql：Percona开发的TPC-C测试工具 具体的调优参数内容较多，具体可参考官方文档，这里介绍一些比较重要的参数： back_log：back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。可以从默认的50升至500 wait_timeout：数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时 max_user_connection: 最大连接数，默认为0无上限，最好设一个合理上限 thread_concurrency：并发线程数，设为CPU核数的两倍 skip_name_resolve：禁止对外部连接进行DNS解析，消除DNS解析时间，但需要所有远程主机用IP访问 key_buffer_size：索引块的缓存大小，增加会提升索引处理速度，对MyISAM表性能影响最大。对于内存4G左右，可设为256M或384M，通过查询show status like 'key_read%'，保证key_reads / key_read_requests在0.1%以下最好 innodb_buffer_pool_size：缓存数据块和索引块，对InnoDB表性能影响最大。通过查询show status like 'Innodb_buffer_pool_read%'，保证(Innodb_buffer_pool_read_requests – Innodb_buffer_pool_reads) / Innodb_buffer_pool_read_requests越高越好 innodb_additional_mem_pool_size：InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率，当过小的时候，MySQL会记录Warning信息到数据库的错误日志中，这时就需要该调整这个参数大小 innodb_log_buffer_size：InnoDB存储引擎的事务日志所使用的缓冲区，一般来说不建议超过32MB query_cache_size：缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失。根据命中率(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大. 可以通过命令show status like 'Qcache_%'查看目前系统Query catch使用大小 read_buffer_size：MySql读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小提高其性能 sort_buffer_size：MySql执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。如果不能，可以尝试增加sort_buffer_size变量的大小 read_rnd_buffer_size：MySql的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。 record_buffer：每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，可能想要增加该值 thread_cache_size：保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的 table_cache：类似于thread_cache_size，但用来缓存表文件，对InnoDB效果不大，主要用于MyISAM 升级硬件 Scale up，这个不多说了，根据MySQL是CPU密集型还是I/O密集型，通过提升CPU和内存、使用SSD，都能显著提升MySQL性能 读写分离 也是目前常用的优化，从库读主库写，一般不要采用双主或多主引入很多复杂性，尽量采用文中的其他方案来提高性能。同时目前很多拆分的解决方案同时也兼顾考虑了读写分离 缓存 缓存可以发生在这些层次： MySQL内部：在系统调优参数介绍了相关设置 数据访问层：比如MyBatis针对SQL语句做缓存，而Hibernate可以精确到单个记录，这里缓存的对象主要是持久化对象Persistence Object 应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对象是数据传输对象Data Transfer Object Web层：针对web页面做缓存 浏览器客户端：用户端的缓存 可以根据实际情况在一个层次或多个层次结合加入缓存。这里重点介绍下服务层的缓存实现，目前主要有两种方式： 直写式（Write Through）：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。这也是当前大多数应用缓存框架如Spring Cache的工作方式。这种实现非常简单，同步好，但效率一般。 回写式（Write Back）：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。 表分区 MySQL在5.1版引入的分区是一种简单的水平拆分，用户需要在建表的时候加上分区参数，对应用是透明的无需修改代码。 对用户来说，分区表是一个独立的逻辑表，但是底层由多个物理子表组成，实现分区的代码实际上是通过对一组底层表的对象封装，但对SQL层来说是一个完全封装底层的黑盒子。MySQL实现分区的方式也意味着索引也是按照分区的子表定义，没有全局索引。 用户的SQL语句是需要针对分区表做优化，SQL条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，可以通过EXPLAIN PARTITIONS来查看某条SQL语句会落在那些分区上，从而进行SQL优化，如下图5条记录落在两个分区上： 1234567mysql&gt; explain partitions select count(1) from user_partition where id in (1,2,3,4,5);+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+| 1 | SIMPLE | user_partition | p1,p4 | range | PRIMARY | PRIMARY | 8 | NULL | 5 | Using where; Using index |+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+1row in set (0.00 sec) 分区的好处是： 可以让单表存储更多的数据 分区表的数据更容易维护，可以通过清楚整个分区批量删除大量数据，也可以增加新的分区来支持新插入的数据。另外，还可以对一个独立分区进行优化、检查、修复等操作 部分查询能够从查询条件确定只落在少数分区上，速度会很快 分区表的数据还可以分布在不同的物理设备上，从而搞笑利用多个硬件设备 可以使用分区表赖避免某些特殊瓶颈，例如InnoDB单个索引的互斥访问、ext3文件系统的inode锁竞争 可以备份和恢复单个分区 分区的限制和缺点： 一个表最多只能有1024个分区 如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来 分区表无法使用外键约束 NULL值会使分区过滤无效 所有分区必须使用相同的存储引擎 分区的类型： RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区 LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择 HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL中有效的、产生非负整数值的任何表达式 KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值 分区适合的场景有： 最适合的场景数据的时间序列性比较强，则可以按时间来分区，如下所示： 1234567891011121314CREATE TABLE members ( firstname VARCHAR(25) NOT NULL, lastname VARCHAR(25) NOT NULL, username VARCHAR(16) NOT NULL, email VARCHAR(35), joined DATE NOT NULL)PARTITION BY RANGE( YEAR(joined) ) ( PARTITION p0 VALUES LESS THAN (1960), PARTITION p1 VALUES LESS THAN (1970), PARTITION p2 VALUES LESS THAN (1980), PARTITION p3 VALUES LESS THAN (1990), PARTITION p4 VALUES LESS THAN MAXVALUE); 查询时加上时间范围条件效率会非常高，同时对于不需要的历史数据能很容的批量删除。 如果数据有明显的热点，而且除了这部分数据，其他数据很少被访问到，那么可以将热点数据单独放在一个分区，让这个分区的数据能够有机会都缓存在内存中，查询时只访问一个很小的分区表，能够有效使用索引和缓存 另外MySQL有一种早期的简单的分区实现 - 合并表（merge table），限制较多且缺乏优化，不建议使用，应该用新的分区机制来替代 垂直拆分 垂直分库是根据数据库里面的数据表的相关性进行拆分，比如：一个数据库里面既存在用户数据，又存在订单数据，那么垂直拆分可以把用户数据放到用户库、把订单数据放到订单库。垂直分表是对数据表进行垂直拆分的一种方式，常见的是把一个多字段的大表按常用字段和非常用字段进行拆分，每个表里面的数据记录数一般情况下是相同的，只是字段不一样，使用主键关联 比如原始的用户表是： 垂直拆分后是： 垂直拆分的优点是： 可以使得行数据变小，一个数据块(Block)就能存放更多的数据，在查询时就会减少I/O次数(每次查询时读取的Block 就少) 可以达到最大化利用Cache的目的，具体在垂直拆分的时候可以将不常变的字段放一起，将经常改变的放一起 数据维护简单 缺点是： 主键出现冗余，需要管理冗余列 会引起表连接JOIN操作（增加CPU开销）可以通过在业务服务器上进行join来减少数据库压力 依然存在单表数据量过大的问题（需要水平拆分） 事务处理复杂 水平拆分 概述 水平拆分是通过某种策略将数据分片来存储，分库内分表和分库两部分，每片数据会分散到不同的MySQL表或库，达到分布式的效果，能够支持非常大的数据量。前面的表分区本质上也是一种特殊的库内分表 库内分表，仅仅是单纯的解决了单一表数据过大的问题，由于没有把表的数据分布到不同的机器上，因此对于减轻MySQL服务器的压力来说，并没有太大的作用，大家还是竞争同一个物理机上的IO、CPU、网络，这个就要通过分库来解决 前面垂直拆分的用户表如果进行水平拆分，结果是： 实际情况中往往会是垂直拆分和水平拆分的结合，即将Users_A_M和Users_N_Z再拆成Users和UserExtras，这样一共四张表 水平拆分的优点是: 不存在单库大数据和高并发的性能瓶颈 应用端改造较少 提高了系统的稳定性和负载能力 缺点是： 分片事务一致性难以解决 跨节点Join性能差，逻辑复杂 数据多次扩展难度跟维护量极大 分片原则 能不分就不分，参考单表优化 分片数量尽量少，分片尽量均匀分布在多个数据结点上，因为一个查询SQL跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量 分片规则需要慎重选择做好提前规划，分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，最近的分片策略为范围分片，枚举分片，一致性Hash分片，这几种分片都有利于扩容 尽量不要在一个事务中的SQL跨越多个分片，分布式事务一直是个不好处理的问题 查询条件尽量优化，尽量避免Select * 的方式，大量数据结果集下，会消耗大量带宽和CPU资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。 通过数据冗余和表分区赖降低跨库Join的可能 这里特别强调一下分片规则的选择问题，如果某个表的数据有明显的时间特征，比如订单、交易记录等，则他们通常比较合适用时间范围分片，因为具有时效性的数据，我们往往关注其近期的数据，查询条件中往往带有时间字段进行过滤，比较好的方案是，当前活跃的数据，采用跨度比较短的时间段进行分片，而历史性的数据，则采用比较长的跨度存储。 总体上来说，分片的选择是取决于最频繁的查询SQL的条件，因为不带任何Where语句的查询SQL，会遍历所有的分片，性能相对最差，因此这种SQL越多，对系统的影响越大，所以我们要尽量避免这种SQL的产生。 解决方案 由于水平拆分牵涉的逻辑比较复杂，当前也有了不少比较成熟的解决方案。这些方案分为两大类：客户端架构和代理架构。 客户端架构 通过修改数据访问层，如JDBC、Data Source、MyBatis，通过配置来管理多个数据源，直连数据库，并在模块内完成数据的分片整合，一般以Jar包的方式呈现 这是一个客户端架构的例子： 可以看到分片的实现是和应用服务器在一起的，通过修改Spring JDBC层来实现 客户端架构的优点是： 应用直连数据库，降低外围系统依赖所带来的宕机风险 集成成本低，无需额外运维的组件 缺点是： 限于只能在数据库访问层上做文章，扩展性一般，对于比较复杂的系统可能会力不从心 将分片逻辑的压力放在应用服务器上，造成额外风险 代理架构 通过独立的中间件来统一管理所有数据源和数据分片整合，后端数据库集群对前端应用程序透明，需要独立部署和运维代理组件 这是一个代理架构的例子： 代理组件为了分流和防止单点，一般以集群形式存在，同时可能需要Zookeeper之类的服务组件来管理 代理架构的优点是： 能够处理非常复杂的需求，不受数据库访问层原来实现的限制，扩展性强 对于应用服务器透明且没有增加任何额外负载 缺点是： 需部署和运维独立的代理中间件，成本高 应用需经过代理来连接数据库，网络上多了一跳，性能有损失且有额外风险。 文章转自 小哈学Java]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vsftpd配置文件详解]]></title>
    <url>%2F2020%2F04%2F02%2Fvsftpd%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[yum安装的vsftpd配置文件如下： 1vim /etc/vsftpd/vsftpd.conf 1）以“#”字符开始的行是注释行。每一行为一个选项设置，格式为“option=value”，注意“=”号两边不能留空白符。 2）除了这个主配置文件外，还可以给特定用户设定个人配置文件 1.与建立FTP链接相关的选项 1.1.监听地址与控制端口 123456789101112listen_address=[IP] # 提供ftp服务的主机IP，单IP主机，不需要使用，多IP主机默认监听所有IP地址。在VSFTPD使用单独(standalone)模式下有效。listen_port=[port] # 提供ftp服务的控制端口号，默认值为21。此选项在standalone模式下生效。port_enable=YES # 是否启用ftp主动模式，默认为YES。如果要在数据连接时取消PORT主动模式时，改为NOconnetc_from_port_20=NO # 以port主动模式进行数据传输时是否使用20端口，默认为NO不使用。但RHEL自带的vsftpd.conf文件中此参数设为YES。ftp_data_port=[port] # 指定ftp数据传输端口值，默认为20。此参数用于主动模式。port_promiscuous=NO # 取消PORT安全检查，默认为NO不取消检查。该检查确保外出的数据只能连接到客户端上。不建议打开pasv_enable=YES # 允许数据传输时使用PASV被动模式。默认为YES允许被动模式pasv_min_port=[minport]pasv_max_port=[maxport] # 设定在PASV被动模式下，建立数据传输所可以使用port范围的下界和上界，默认为0不限制。 # 把端口范围设在比较高的一段范围内，比如50000-60000，将有助于安全性的提高。pasv_promiscuous=NO # 是否关闭PASV模式的安全检查，默认值为NO不关闭。该检查确保数据连接和控制连接是来自同一个IP地址。不建议打开，此选项唯一合理的用法是存在于由安全隧道方案构成的组织中。pasv_address=[IP] # 此选项为一个数字IP地址，作为PASV命令的响应。默认值为none，即地址是从呼入的连接套接字(incomingconnectdsocket)中获取。 1.2.关于ftp服务的ASCII模式 默认情况下，VSFTPD使用二进制传输数据，禁止使用ASCII传输模式。 如果FTP客户端使用ascii命令，指明要使用ASCII模式，VSFTPD表面上接受了ascii命令，但在实际传输文件时，还是使用二进制方式，就会出现乱码文件 12ascii_upload_enable=NO #控制是否允许使用ascii模式上传文件，默认为NO不允许。ascii_download_enable=NO # 控制是否允许使用ascii模式下载文件，默认为NO不允许。 2.关于性能与负载控制 1234567891011idle_session_timeout=60 # 设置用户会话的空闲中断时间，单位为秒，默认值为300。data_connection_timeout=120 # 设置空闲的数据连接的中断时间。默认值为300秒。accept_timeout=60 # 接受建立联机的超时设定，单位为秒。默认值为60。connect_timeout=60 # 响应PORT方式的数据联机的超时设定，单位为秒。默认值为60。 # 以上两个选项针对客户端的，将使客户端空闲1分钟后自动中断连接，并在中断1分钟后自动激活连接。max_clients=200 # 指明服务器总的客户并发连接数为200，默认值为0，表示不限最大连接数。此参数在使用standalone模式下有效max_per_ip=3 # 指明每个IP地址的并发连接数，默认值为0不限制。该设置会影响到象网际快车这类的多进程下载软件。此参数在使用standalone模式下有效local_max_rate=50000 # 设置本地用户的最大传输速率限制（50kbytes/sec），以Bytes/s为单位。默认0不限制。此选项可以为指定用户单独设置anon_max_rate=30000 # 设定匿名用户的最大数据传输速度value，以Bytes/s为单位。默认0不限制pasv_min_port=50000 # 设置被动模式客户端连接时的端口范围，默认为0不限制pasv_max_port=60000 3.用户选项 VSFTPD的用户分为三类：匿名用户、本地用户（localuser）以及虚拟用户 3.1.匿名用户 123456789anonymous_enable=YES|NO # 控制是否允许匿名用户登录，默认值为YES允许匿名用户登录。ftp_username= # 匿名用户所使用的系统用户名。默认此参数在配置文件中不出现，值为ftp。no_anon_password=NO # 控制匿名用户登入时是否需要密码，默认值为NO需要密码。deny_email_enable=NO # 拒绝在banned_email_file指定的文件中所列出的email地址进行登录的匿名用户。默认值为NO关闭。这对于阻击某些Dos攻击有效。如果开启需要追加以下配置banned_email_file=/etc/banned_emails.conf # 当匿名用户使用banned_email_file文件中所列出的e-mail进行登录时，被拒绝指定包含被拒绝的e-mail地址的文件，默认文件为/etc/vsftpd.banned_emails。anon_root= # 匿名用户的根目录，默认为/var/ftp/，主配置文件中默认无此项。anon_world_readable_only=YES # 默认值为YES只允许匿名用户下载可阅读的文件。NO允许匿名用户浏览整个服务器的文件anon_upload_enable=NO # 是否允许匿名用户上传文件，默认NO不允许。 除了anon_upload_enable这个参数外，匿名用户要能上传文件，还需要两个条件： 1）write_enable参数为YES; 2）在文件系统上，FTP匿名用户对某个目录有写权限 1234anon_mkdir_write_enable=NO # 是否允许匿名用户创建新目录，默认为NO不允许，同时FTP匿名用户必需对新目录的上层目录拥有写权限。anon_other_write_enable=NO # 匿名用户是否拥有除了上传和新建目录之外的其他权限，如删除、更名等。默认为NO不拥有chown_uploads=NO # 是否修改匿名用户所上传文件的所有权。默认值为NO不修改。如果改为YES匿名用户所上传的文件的所有权将改为另外一个不同的用户所有chown_username=whoever # 指定拥有匿名用户上传文件所有权的用户。此参数与chown_uploads联用。不推荐使用root用户。 3.2.本地用户 在使用FTP服务的用户中，除了匿名用户外，还有一类在FTP服务器所属主机上拥有账号的用户。VSFTPD中称此类用户为本地用户（localuser），等同于其他FTP服务器中的real用户。 123local_enable=YES # 本地系统用户是否可以登陆，默认值为YES。local_root= # 定义所有本地用户的根目录。默认为空，本地用户登录到自己的宿主目录user_config_dir=/etc/vsftpd/user.d # 定义用户个人配置文件所在的目录，配置文件名为用户名，配置格式与vsftpd.conf相同。默认为无不单独设置用户权限，虚拟用户也用这个 3.3.虚拟用户 12guest_enable=NO # 若是启动这项功能，所有的非匿名登入者都视为guest，默认值为NO关闭。如果要使用ftp虚拟用户需要启用guest_username= # 定义VSFTPD的guest用户在系统中的用户名。默认值为ftp，在使用ftp虚拟用户时建议自定义，例如ftpvuser 4.安全措施 4.1.用户登录控制 123456/etc/vsftpd/ftpusers # 改配置文件中的用户禁止登录FTP服务器。这个机制是在/etc/pam.d/vsftpd中默认设置的。pam_service_name=vsftpd # 指定vsftpd进行PAM认证时所使用的PAM配置文件名，默认值是vsftpd，默认PAM配置文件是/etc/pam.d/vsftpd，使用ftp虚拟用户时需要更改userlist_enable=NO # 是否通过userlist_file列表控制可登陆的用户，默认为NO不启用，如果启用，列表中的用户默认拒绝登录FTP服务器，在输入用户名后，不提示输入密码userlist_deny=YES # 决定禁止还是允许userlist_file指定文件中的用户登录FTP服务器，默认为YES禁止文件中的用户登录，此选项在userlist_enable选项启动后才生效，如果要允许在文件中的用户登录FTP服务器需要改为NOuserlist_file=/etc/vsftpd/user_list # userlist_enable选项指定的用户列表的文件。默认为/etc/vsftpd/user_list。tcp_wrappers=YES # 在vsftpd中使用TCP_Wrappers封装数据，默认值为YES。 4.2.目录访问控制 123456789chroot_list_enable=NO # 是否锁定用户在其宿主目录中，默认值为NO不锁定。具体的用户在chroot_list_file参数所指定的文件中列出。chroot_list_file=/etc/vsftpd/chroot_list # 指定要锁定在宿主目录中的用户，默认不设置。一行一用户，通常为/etc/vsftpd/chroot_listchroot_local_users=NO # 将本地用户锁定在其宿主目录中，默认值为NO不锁定。注意：当chroot_local_users被激活时，chroot_list_enable和chroot_local_users参数的作用将发生变化，chroot_list_file所指定文件中的用户将不被锁定在自家目录。可能带来安全上的冲突，特别是当用户拥有上传、shell访问等权限时。passwd_chroot_enable=NO # 当此选项需与chroot_local_user配合，chroot()容器的位置可以在每个用户的基础上指定。每个用户的容器来源于/etc/passwd中每个用户的自家目录字段。默认值为NO。 4.3.文件操作控制 1234567hide_ids=YES|NO # 是否隐藏文件的所有者和组信息。默认值为NO不隐藏，如果为YES，当用户使用&quot;ls -al&quot;之类的指令时，在目录列表中所有文件的拥有者和组信息都显示为ftpls_recurse_enable=YES|NO # 是否允许使用&quot;ls-R&quot;指令，默认值为NO不允许。如果在一个大型FTP站点的根目录下使用&quot;ls-R&quot;会消耗大量系统资源。write_enable=YES|NO # 是否允许使用修改文件系统的FTP的指令，默认为NO不允许，比如STOR、DELE、RNFR、RNTO、MKD、RMD、APPE以及SITEsecure_chroot_dir= # 安全沙箱目录，指向一个ftp用户无写权限的空目录，默认为/usr/share/empty。当vsftpd不需要访问文件系统时，这个目录将被作为一个安全的容器，用户将被限制在此目录中。anon_umask= # 匿名用户新增文件的umask数值。默认值为077。file_open_mode= # 上传文件的权限，默认值为0666。与chmod所使用的数值相同。如果希望上传的文件可以执行，设此值为0777。local_umask= # 本地用户新增文件时的umask数值，默认值为077。如果希望新增的文件他人可以访问的话，修改为022 5.提示信息 12345ftpd_banner=welcome # 此参数定义了登录欢迎语，预设值为无，可修改banner_file=/etc/vsftpd/banner_file # 当用户登录时会显示此文件中的内容，通常为欢迎话语或是说明。默认值为无。 # 与ftpd_banner相比，banner_file是文本文件的形式，而ftpd_banner是字串格式。banner_file选项将取代ftpd_banner选项。dirmessage_enable=YES # 特定目录的提示信息，默认为YES启用。当用户进入指定目录，如果该目录下存在message_file指定的文件，则显示出现此文档的内容，通常这个文档会放置欢迎话语，或是对该目录的说明。message_file= # dirmessage_enable选项启用时生效，指定提示内容的文档。默认为.message，以该扩展名结尾的文件 6.日志设置 1234xferlog_enable=NO # 是否启用一个日志文件，记录上传和下载，默认为NO不启用，该日志文件由xferlog_file选项指定xferlog_file=/var/log/vsftpd.log # 记录ftp日志。默认为/var/log/vsftpd.logxferlog_std_format=NO # 日志文件格式是否使用xferlog的标准格式，默认为NO不使用。默认的日志格式更为可读性，使用xferlog格式可以使用已经存在的传输统计生成器。log_ftp_protocol=NO # 是否记录所有的FTP请求和响应到日志中，这个选项一般用于调试，默认为NO不记录。使用此选项时xferlog_std_format不能被激活 7.其他设置 123456setproctitle_enable=NO # 是否在系统进程列表中显示每个会话(session)的状态，默认为NO不显示。包括挂起、下载等text_userdb_names=No # 用户使用ls -al命令时，列表信息是否显示拥有者名称而不是UID，默认为NO不显示use_localtime=NO # vsftpd显示目录列表时是否使用服务器本地时区的时间。默认为NO显示GMT时间，建议修改为YES。由ftp命令“MDTM”返回的时间值也受此选项影响。check_shell=YES # 本地用户登录时vsftpd是否检查/etc/shells文件以寻找一个有效的用户shell。默认为YES。此选项仅对不使用PAM方式的VSFTPD生效。nopriv_user=nobody # 指定一个专用的除nobody以外的用户，当VSFTPD不想要什么权限时，使用此用户身份。默认值为nobody，如果使用建议修改，因为在大多数的机器上，nobody用户被用于大量重要的事情pam_service_name= # 指明vsftpd使用用PAM验证服务时的PAM配置文件名。默认值为ftp。 文章转载来自https://www.cnblogs.com/tssc/p/9592600.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vsftpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不就是SELECT COUNT语句吗，竟然能被面试官虐的体无完肤]]></title>
    <url>%2F2020%2F03%2F26%2F%E4%B8%8D%E5%B0%B1%E6%98%AFSELECT-COUNT%E8%AF%AD%E5%8F%A5%E5%90%97%EF%BC%8C%E7%AB%9F%E7%84%B6%E8%83%BD%E8%A2%AB%E9%9D%A2%E8%AF%95%E5%AE%98%E8%99%90%E7%9A%84%E4%BD%93%E6%97%A0%E5%AE%8C%E8%82%A4%2F</url>
    <content type="text"><![CDATA[文章来自 Hollis 数据库查询相信很多人都不陌生，所有经常有人调侃程序员就是CRUD专员，这所谓的CRUD指的就是数据库的增删改查。 在数据库的增删改查操作中，使用最频繁的就是查询操作。而在所有查询操作中，统计数量操作更是经常被用到。 关于数据库中行数统计，无论是MySQL还是Oracle，都有一个函数可以使用，那就是COUNT。 但是，就是这个常用的COUNT函数，却暗藏着很多玄机，尤其是在面试的时候，一不小心就会被虐。不信的话请尝试回答下以下问题： 1、COUNT有几种用法？ 2、COUNT(字段名)和COUNT()的查询结果有什么不同？ 3、COUNT(1)和COUNT()之间有什么不同？ 4、COUNT(1)和COUNT()之间的效率哪个更高？ 5、为什么《阿里巴巴Java开发手册》建议使用COUNT() 6、MySQL的MyISAM引擎对COUNT()做了哪些优化？ 7、MySQL的InnoDB引擎对COUNT()做了哪些优化？ 8、上面提到的MySQL对COUNT()做的优化，有一个关键的前提是什么？ 9、SELECT COUNT() 的时候，加不加where条件有差别吗？ 10、COUNT(*)、COUNT(1)和COUNT(字段名)的执行过程是怎样的？ 以上10道题，如果您可以全部准确无误的回答的话，那说明你真的很了解COUNT函数了，如果有哪些知识点是不了解的，那么本文正好可以帮你答疑解惑。 认识COUNT 关于COUNT函数，在MySQL官网中有详细介绍： 简单翻译一下： 1、COUNT(expr) ，返回SELECT语句检索的行中expr的值不为NULL的数量。结果是一个BIGINT值。 2、如果查询结果没有命中任何记录，则返回0 3、但是，值得注意的是，COUNT(*) 的统计结果中，会包含值为NULL的行数。 即以下表记录 12345678create table #bla(id int,id2 int)insert #bla values(null,null)insert #bla values(1,null)insert #bla values(null,1)insert #bla values(1,null)insert #bla values(null,1)insert #bla values(1,null)insert #bla values(null,null) 使用语句count(*),count(id),count(id2)查询结果如下： 12select count(*),count(id),count(id2)from #bla results 7 3 2 除了COUNT(id)和COUNT(*)以外，还可以使用COUNT(常量)（如COUNT(1)）来统计行数，那么这三条SQL语句有什么区别呢？到底哪种效率更高呢？为什么《阿里巴巴Java开发手册》中强制要求不让使用 COUNT(列名)或 COUNT(常量)来替代 COUNT(*)呢？ COUNT(列名)、COUNT(常量)和COUNT(*)之间的区别 前面我们提到过COUNT(expr)用于做行数统计，统计的是expr不为NULL的行数，那么COUNT(列名)、 COUNT(常量) 和 COUNT()这三种语法中，expr分别是列名、 常量 和 。 那么列名、 常量 和 *这三个条件中，常量 是一个固定值，肯定不为NULL。*可以理解为查询整行，所以肯定也不为NULL，那么就只有列名的查询结果有可能是NULL了。 所以， COUNT(常量) 和 COUNT(*)表示的是直接查询符合条件的数据库表的行数。而COUNT(列名)表示的是查询符合条件的列的值不为NULL的行数。 除了查询得到结果集有区别之外，COUNT(*)相比COUNT(常量) 和 COUNT(列名)来讲，COUNT(*)是SQL92定义的标准统计行数的语法，因为他是标准语法，所以MySQL数据库对他进行过很多优化。 SQL92，是数据库的一个ANSI/ISO标准。它定义了一种语言（SQL）以及数据库的行为（事务、隔离级别等）。 COUNT(*)的优化 前面提到了COUNT(*)是SQL92定义的标准统计行数的语法，所以MySQL数据库对他进行过很多优化。那么，具体都做过哪些事情呢？ 这里的介绍要区分不同的执行引擎。MySQL中比较常用的执行引擎就是InnoDB和MyISAM。 MyISAM和InnoDB有很多区别，其中有一个关键的区别和我们接下来要介绍的COUNT(*)有关，那就是MyISAM不支持事务，MyISAM中的锁是表级锁；而InnoDB支持事务，并且支持行级锁。 因为MyISAM的锁是表级锁，所以同一张表上面的操作需要串行进行，所以，MyISAM做了一个简单的优化，那就是它可以把表的总行数单独记录下来，如果从一张表中使用COUNT(*)进行查询的时候，可以直接返回这个记录下来的数值就可以了，当然，前提是不能有where条件。 MyISAM之所以可以把表中的总行数记录下来供COUNT(*)查询使用，那是因为MyISAM数据库是表级锁，不会有并发的数据库行数修改，所以查询得到的行数是准确的。 但是，对于InnoDB来说，就不能做这种缓存操作了，因为InnoDB支持事务，其中大部分操作都是行级锁，所以可能表的行数可能会被并发修改，那么缓存记录下来的总行数就不准确了。 但是，InnoDB还是针对COUNT(*)语句做了些优化的。 在InnoDB中，使用COUNT(*)查询行数的时候，不可避免的要进行扫表了，那么，就可以在扫表过程中下功夫来优化效率了。 从MySQL 8.0.13开始，针对InnoDB的 SELECT COUNT(*) FROM tbl_name 语句，确实在扫表的过程中做了一些优化。前提是查询语句中不包含WHERE或GROUP BY等条件。 我们知道，COUNT(*)的目的只是为了统计总行数，所以，他根本不关心自己查到的具体值，所以，他如果能够在扫表的过程中，选择一个成本较低的索引进行的话，那就可以大大节省时间。 我们知道，InnoDB中索引分为聚簇索引（主键索引）和非聚簇索引（非主键索引），聚簇索引的叶子节点中保存的是整行记录，而非聚簇索引的叶子节点中保存的是该行记录的主键的值。 所以，相比之下，非聚簇索引要比聚簇索引小很多，所以MySQL会优先选择最小的非聚簇索引来扫表。所以，当我们建表的时候，除了主键索引以外，创建一个非主键索引还是有必要的。 至此，我们介绍完了MySQL数据库对于COUNT(*)的优化，这些优化的前提都是查询语句中不包含WHERE以及GROUP BY条件。 COUNT(*)和COUNT(1) 介绍完了COUNT()，接下来看看COUNT(1)，对于，这二者到底有没有区别，网上的说法众说纷纭。 有的说COUNT()执行时会转换成COUNT(1)，所以COUNT(1)少了转换步骤，所以更快。 还有的说，因为MySQL针对COUNT(*)做了特殊优化，所以COUNT(*)更快。 那么，到底哪种说法是对的呢？看下MySQL官方文档是怎么说的： InnoDB handles SELECT COUNT(*) and SELECT COUNT(1) operations in the same way. There is no performance difference. 画重点：same way , no performance difference。所以，对于COUNT(1)和COUNT(*)，MySQL的优化是完全一样的，根本不存在谁比谁快！ 那既然COUNT(*)和COUNT(1)一样，建议用哪个呢？ 建议使用COUNT(*)！因为这个是SQL92定义的标准统计行数的语法，而且本文只是基于MySQL做了分析，关于Oracle中的这个问题，也是众说纷纭的呢。 COUNT(字段) 最后，就是我们一直还没提到的COUNT(字段)，他的查询就比较简单粗暴了，就是进行全表扫描，然后判断指定字段的值是不是为NULL，不为NULL则累加。 相比COUNT(*)，COUNT(字段)多了一个步骤就是判断所查询的字段是否为NULL，所以他的性能要比COUNT(*)慢。 总结 本文介绍了COUNT函数的用法，主要用于统计表行数。主要用法有COUNT(*)、COUNT(字段)和COUNT(1)。 因为COUNT(*)是SQL92定义的标准统计行数的语法，所以MySQL对他进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本。当然，这些优化的前提都是没有进行where和group的条件查询。 在InnoDB中COUNT(*)和COUNT(1)实现上没有区别，而且效率一样，但是COUNT(字段)需要进行字段的非NULL判断，所以效率会低一些。 因为COUNT(*)是SQL92定义的标准统计行数的语法，并且效率高，所以请直接使用COUNT(*)查询表的行数！ 参考资料： https://dev.mysql.com/doc/refman/8.0/en/group-by-functions.html#function_count 《极客时间——MySQL实战45讲》]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
        <tag>COUNT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle索引]]></title>
    <url>%2F2020%2F03%2F23%2FOracle%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引的作用相当于图书的目录，可以根据目录中的页码快速找到所需的内容。 在没有创建索引之前，如果要按照用户名字段username全表查询一条用户数据，就要必须在全表都搜索一遍；在username上创建索引，Oracle会对全表进行一次搜索，将每条记录的username按照顺序排序，然后构建索引条目（name和rowid），存储到索引段中。在接下来查找username等于某个值时即可直接查找到相应的地方。 文章来自 FREE教程 既然我们都知道建立索引有利于查询速率的提升，那是不是所有字段都可以加上索引。这是万万不行的，建立索引不仅仅要浪费空间来存储索引表，当数据量较少时，直接查询数据比经过查询索引表再定位到表数据的速度更快。索引可以提高查询的效率，但是在数据增删改时需要更新索引，因此索引对增删改时会有负面影响。所以要根据实际情况， 考虑好再建立索引。 何时建立索引 那何时建立索引，下面大概介绍几点，其余的得在实际应用和开发过程中，酌情考虑： 1、Oracle 数据库会为表的主键和包含唯一约束的列自动创建索引，所以在建立唯一约束时，可以考虑该列是否必要建立。是否经常要作为查询条件。 2、如果某个表的数据量较大（十几二十万以上），某列经常作为where的查询条件，并且检索的出来的行数经常是小于总表的5%，那该列可以考虑建立索引。 3、对于两表连接的字段，应该考虑建立索引。如果经常在某表的一个字段进行Order By 则也经过进行索引。 4、不应该在小表上建立索引。上面也说过，小表之间查询的数据会比建立索引的查询速度更快，但是在某些字段，如性别：只有男、女和未知三种数据时，可以考虑位图索引，可以增加查询效率。 5、经常进行DML操作，即经常进行增删改的操作的表，创建表索引时就要权衡一下，因为建索引会导致进行DML操作时速度变慢。所以可以根据实际情况，选择某些字段建立索引，而不能盲目乱建。 索引的类别 适当的使用索引可以提高数据检索速度，那Oracle有哪些类型的索引呢？ 1、b-tree索引：Oracle数据中最常见的索引，就是b-tree索引，create index创建的normal就是b-tree索引，没有特殊的必须应用在哪些数据上。 2、bitmap位图索引：位图索引经常应用于列数据只有几个枚举值的情况，比如上面说到过的性别字段，或者我们经常开发中应用的代码字段。这个时候使用bitmap位图索引，查询效率将会最快。 3、函数索引：比如经常对某个字段做查询的时候经常是带函数操作的，那么此时建一个函数索引就有价值了。例如：trim（列名）或者substr(列名)等等字符串操作函数，这个时候可以建立函数索引来提升这种查询效率。 4、hash索引：hash索引可能是访问数据库中数据的最快方法，但它也有自身的缺点。创建hash索引必须使用hash集群，相当于定义了一个hash集群键，通过这个集群键来告诉oracle来存储表。因此，需要在创建hash集群的时候指定这个值。存储数据时，所有相关集群键的行都存储在一个数据块当中，所以只要定位到hash键，就能快速定位查询到数据的物理位置。 5、reverse反向索引：这个索引不经常使用到，但是在特定的情况下，是使用该索引可以达到意想不到的效果。如：某一列的值为{10000,10001,10021,10121,11000,....}，假如通过b-tree索引，大部分都密集发布在某一个叶子节点上，但是通过反向处理后的值将变成{00001,10001,12001,12101,00011,...}，很明显的发现他们的值变得比较随机，可以比较平均的分部在各个叶子节点上，而不是之前全部集中在某一个叶子节点上，这样子就可大大提高检索的效率。 6、分区索引和分区表的全局索引：这两个索引是应用在分区表上面的，前者的分区索引是对分区表内的单个分区进行数据索引，后者是对分区表的全表进行全局索引。分区表的介绍，可以后期再做单独详解，这里就不累述了。 创建索引 语法结构： 123456789101112create[unique]|[bitmap] index index_name --UNIQUE表示唯一索引、BITMAP位图索引on table_name(column1,column2...|[express])--express表示函数索引[tablespace tab_name] --tablespace表示索引存储的表空间[pctfree n1] --索引块的空闲空间n1[storage --存储块的空间 ( initial 64K --初始64k next 1M minextents 1 maxextents unlimited )]; Oracle要求创建索引的列不能超过32列 语法解析： 1、UNIQUE:指定索引列上的值必须是唯一的。称为唯一索引，BITMAP表示位图索引。 2、index_name：指定索引名。 3、tabl_name：指定要为哪个表创建索引。 4、column_name：指定要对哪个列创建索引。我们也可以对多列创建索引，这种索引称为组合索引。也可以是函数表达式，这种就是函数索引。 修改索引： 1、重命名索引： 1alter index index_old rename to index_new;--重新命名索引 2、合并索引、重新构造索引：我们索引建好后，经过很长一段时间的使用，索引表中存储的空间会产生一些碎片，导致索引的查询效率会有所下降，这个时候可以合并索引，原理是按照索引规则重新分类存储一下，或者也可以选择删除索引重新构造索引。 12alter index index_name coalesce;--合并索引alter index index_name rebuild;--重新构造 删除索引： 1drop index index_name; 查看索引： 1234567select t.INDEX_NAME,--索引名字 t.index_type,--索引类型 t.TABLESPACE_NAME,--表空间 t.status,--状态 t.UNIQUENESS--是否唯一索引 from all_indexes T where t.INDEX_NAME='index_name'; 案例分析： 案例1、学生信息表（stuinfo）创建的时候就对学号（stuid）设置了主键（PK_STUINFO），当我们学生信息表数据量大的情况下，我们明显发现班号（classno）需要一个索引，不仅仅是用来关联班级信息表（class）、而且经常作为查询条件，因此创建脚本如下： 123456789101112create index STUDENT.IDX_STUINFO_CLASSNO on STUDENT.STUINFO (CLASSNO) tablespace USERS pctfree 10 initrans 2 maxtrans 255 storage ( initial 64K next 1M minextents 1 maxextents unlimited ); 案例2、对于学生信息我们经常用性别作为统计条件进行对学生信息进行统计，因此我们可以在性别（sex）建立一个位图索引进行查询优化。代码如下： 123456789101112create bitmap index STUDENT.IDX_STUINFO_SEX on STUDENT.STUINFO (SEX) tablespace USERS pctfree 10 initrans 2 maxtrans 255 storage ( initial 64K next 1M minextents 1 maxextents unlimited ); 查询一下三种索引的状态： 12345678select t.INDEX_NAME, t.index_type, t.TABLESPACE_NAME, t.status, t.UNIQUENESS from all_indexes T where t.TABLE_NAME='STUINFO' AND T.OWNER='STUDENT' 结果如下：]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>索引</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解线程安全]]></title>
    <url>%2F2019%2F05%2F28%2F%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[不是线程的安全 面试官问：“什么是线程安全”，如果你不能很好的回答，那就请往下看吧。 论语中有句话叫“学而优则仕”，相信很多人都觉得是“学习好了可以做官”。然而，这样理解却是错的。切记望文生义。 同理，“线程安全”也不是指线程的安全，而是指内存的安全。为什么如此说呢？这和操作系统有关。 目前主流操作系统都是多任务的，即多个进程同时运行。为了保证安全，每个进程只能访问分配给自己的内存空间，而不能访问别的进程的，这是由操作系统保障的。 在每个进程的内存空间中都会有一块特殊的公共区域，通常称为堆（内存）。进程内的所有线程都可以访问到该区域，这就是造成问题的潜在原因。 假设某个线程把数据处理到一半，觉得很累，就去休息了一会，回来准备接着处理，却发现数据已经被修改了，不是自己离开时的样子了。可能被其它线程修改了。 比如把你住的小区看作一个进程，小区里的道路/绿化等就属于公共区域。你拿1万块钱往地上一扔，就回家睡觉去了。睡醒后你打算去把它捡回来，发现钱已经不见了。可能被别人拿走了。 因为公共区域人来人往，你放的东西在没有看管措施时，一定是不安全的。内存中的情况亦然如此。 所以线程安全指的是，在堆内存中的数据由于可以被任何线程访问到，在没有限制的情况下存在被意外修改的风险。 即堆内存空间在没有保护机制的情况下，对多线程来说是不安全的地方，因为你放进去的数据，可能被别的线程“破坏”。 那我们该怎么办呢？解决问题的过程其实就是一个取舍的过程，不同的解决方案有不同的侧重点。 私有的东西就不该让别人知道 现实中很多人都会把1万块钱藏着掖着，不让无关的人知道，所以根本不可能扔到大马路上。因为这钱是你的私有物品。 在程序中也是这样的，所以操作系统会为每个线程分配属于它自己的内存空间，通常称为栈内存，其它线程无权访问。这也是由操作系统保障的。 如果一些数据只有某个线程会使用，其它线程不能操作也不需要操作，这些数据就可以放入线程的栈内存中。较为常见的就是局部变量。 123456789double avgScore(double[] scores) &#123; double sum = 0; for (double score : scores) &#123; sum += score; &#125; int count = scores.length; double avg = sum / count; return avg;&#125; 这里的变量sum，count，avg都是局部变量，它们都会被分配在线程栈内存中。 假如现在A线程来执行这个方法，这些变量会在A的栈内存分配。与此同时，B线程也来执行这个方法，这些变量也会在B的栈内存中分配。 也就是说这些局部变量会在每个线程的栈内存中都分配一份。由于线程的栈内存只能自己访问，所以栈内存中的变量只属于自己，其它线程根本就不知道。 就像每个人的家只属于自己，其他人不能进来。所以你把1万块钱放到家里，其他人是不会知道的。且一般还会放到某个房间里，而不是仍在客厅的桌子上。 所以把自己的东西放到自己的私人地盘，是安全的，因为其他人无法知道。而且越隐私的地方越好。 大家不要抢，人人有份 相信聪明的你已经发现，上面的解决方案是基于“位置”的。因为你放东西的“位置”只有你自己知道（或能到达），所以东西是安全的，因此这份安全是由“位置”来保障的。 在程序里就对应于方法的局部变量。局部变量之所以是安全的，就是因为定义它的“位置”是在方法里。这样一来安全是达到了，但是它的使用范围也就被限制在这个方法里了，其它方法想用也不用了啦。 现实中往往会有一个变量需要多个方法都能够使用的情况，此时定义这个变量的“位置”就不能在方法里面了，而应该在方法外面。即从（方法的）局部变量变为（类的）成员变量，其实就是“位置”发生了变化。 那么按照主流编程语言的规定，类的成员变量不能再分配在线程的栈内存中，而应该分配在公共的堆内存中。其实也就是变量在内存中的“位置”发生了变化，由一个私有区域来到了公共区域。因此潜在的安全风险也随之而来。 那怎么保证在公共区域的东西安全呢？答案就是，大家不要抢，人人有份。设想你在街头免费发放矿泉水，来了1万人，你却只有1千瓶水，结果可想而知，一拥而上，场面失守。但如果你有10万瓶水，大家一看，水多着呢，不用着急，一个个排着队来，因为肯定会领到。 东西多了，自然就不值钱了，从另一个角度来说，也就安全了。大街上的共享单车，现在都很安全，因为太多了，到处都是，都长得一样，所以连搞破坏的人都放弃了。因此要让一个东西安全，就疯狂的copy它吧。 回到程序里，要让公共区域堆内存中的数据对于每个线程都是安全的，那就每个线程都拷贝它一份，每个线程只处理自己的这一份拷贝而不去影响别的线程的，这不就安全了嘛。相信你已经猜到了，我要表达的就是ThreadLocal类了。 123456789101112131415161718192021222324252627282930313233343536373839class StudentAssistant &#123; ThreadLocal&lt;String&gt; realName = new ThreadLocal&lt;&gt;(); ThreadLocal&lt;Double&gt; totalScore = new ThreadLocal&lt;&gt;(); String determineDegree() &#123; double score = totalScore.get(); if (score &gt;= 90) &#123; return "A"; &#125; if (score &gt;= 80) &#123; return "B"; &#125; if (score &gt;= 70) &#123; return "C"; &#125; if (score &gt;= 60) &#123; return "D"; &#125; return "E"; &#125; double determineOptionalcourseScore() &#123; double score = totalScore.get(); if (score &gt;= 90) &#123; return 10; &#125; if (score &gt;= 80) &#123; return 20; &#125; if (score &gt;= 70) &#123; return 30; &#125; if (score &gt;= 60) &#123; return 40; &#125; return 60; &#125;&#125; 这个学生助手类有两个成员变量，realName和totalScore，都是ThreadLocal类型的。每个线程在运行时都会拷贝一份存储到自己的本地。 A线程运行的是“张三”和“90”，那么这两个数据“张三”和“90”是存储到A线程对象（Thread类的实例对象）的成员变量里去了。假设此时B线程也在运行，是“李四”和“85”，那么“李四”和“85”这两个数据是存储到了B线程对象（Thread类的实例对象）的成员变量里去了。 线程类（Thread）有一个成员变量，类似于Map类型的，专门用于存储ThreadLocal类型的数据。从逻辑从属关系来讲，这些ThreadLocal数据是属于Thread类的成员变量级别的。从所在“位置”的角度来讲，这些ThreadLocal数据是分配在公共区域的堆内存中的。 说的直白一些，就是把堆内存中的一个数据复制N份，每个线程认领1份，同时规定好，每个线程只能玩自己的那份，不准影响别人的。 需要说明的是这N份数据都还是存储在公共区域堆内存里的，经常听到的“线程本地”，是从逻辑从属关系上来讲的，这些数据和线程一一对应，仿佛成了线程自己“领地”的东西了。其实从数据所在“位置”的角度来讲，它们都位于公共的堆内存中，只不过被线程认领了而已。这一点我要特地强调一下。 其实就像大街上的共享单车。原来只有1辆，大家抢着骑，老出问题。现在从这1辆复制出N辆，每人1辆，各骑各的，问题得解。共享单车就是数据，你就是线程。骑行期间，这辆单车从逻辑上来讲是属于你的，从所在位置上来讲还是在大街上这个公共区域的，因为你发现每个小区大门口都贴着“共享单车，禁止入门”。哈哈哈哈。 共享单车是不是和ThreadLocal很像呀。再重申一遍，ThreadLocal就是，把一个数据复制N份，每个线程认领一份，各玩各的，互不影响。 只能看，不能摸 放在公共区域的东西，只是存在潜在的安全风险，并不是说一定就不安全。有些东西虽然也在公共区域放着，但也是十分安全的。比如你在大街上放一个上百吨的石头雕像，就非常安全，因为大家都弄不动它。 再比如你去旅游时，经常发现一些珍贵的东西，会被用铁栅栏围起来，上面挂一个牌子，写着“只能看，不能摸”。当然可以国际化一点，“only look，don't touch”。这也是很安全的，因为光看几眼是不可能看坏的。 回到程序里，这种情况就属于，只能读取，不能修改。其实就是常量或只读变量，它们对于多线程是安全的，想改也改不了。 1234class StudentAssistant &#123; final double passScore = 60;&#125; 比如把及格分数设定为60分，在前面加上一个final，这样所有线程都动不了它了。这就很安全了。 小节一下：以上三种解决方案，其实都是在“耍花招”。 第一种，找个只有自己知道的地方藏起来，当然安全了。 第二种，每人复制1份，各玩各的，互不影响，当然也安全了。 第三种，更狠了，直接规定，只能读取，禁止修改，当然也安全了。 是不是都在“避重就轻”呀。如果这三种方法都解决不了，该怎么办呢？Don't worry，just continue reading。 没有规则，那就先入为主 前面给出的三种方案，有点“理想化”了。现实中的情况其实是非常混乱嘈杂的，没有规则的。 比如在中午高峰期你去饭店吃饭，进门后发现只剩一个空桌子了，你心想先去点餐吧，回来就坐这里吧。当你点完餐回来后，发现已经被别人捷足先登了。 因为桌子是属于公共区域的物品，任何人都可以坐，那就只能谁先抢到谁坐。虽然你在人群中曾多看了它一眼，但它并不会记住你容颜。 解决方法就不用我说了吧，让一个人在那儿看着座位，其它人去点餐。这样当别人再来的时候，你就可以理直气壮的说，“不好意思，这个座位，我，已经占了”。 我再次相信聪明的你已经猜到了我要说的东西了，没错，就是（互斥）锁。 回到程序里，如果公共区域（堆内存）的数据，要被多个线程操作时，为了确保数据的安全（或一致）性，需要在数据旁边放一把锁，要想操作数据，先获取锁再说吧。 假设一个线程来到数据跟前一看，发现锁是空闲的，没有人持有。于是它就拿到了这把锁，然后开始操作数据，干了一会活，累了，就去休息了。 这时，又来了一个线程，发现锁被别人持有着，按照规定，它不能操作数据，因为它无法得到这把锁。当然，它可以选择等待，或放弃，转而去干别的。 第一个线程之所以敢大胆的去睡觉，就是因为它手里拿着锁呢，其它线程是不可能操作数据的。当它回来后继续把数据操作完，就可以把锁给释放了。锁再次回到空闲状态，其它线程就可以来抢这把锁了。还是谁先抢到锁谁操作数据。 1234567891011121314151617class ClassAssistant &#123; double totalScore = 60; final Lock lock = new Lock(); void addScore(double score) &#123; lock.obtain(); totalScore += score; lock.release(); &#125; void subScore(double score) &#123; lock.obtain(); totalScore -= score; lock.release(); &#125;&#125; 假定一个班级的初始分数是60分，这个班级抽出10名学生来同时参加10个不同的答题节目，每个学生答对一次为班级加上5分，答错一次减去5分。因为10个学生一起进行，所以这一定是一个并发情形。 因此加分和减分这两个方法被并发的调用，它们共同操作总分数。为了保证数据的一致性，需要在每次操作前先获取锁，操作完成后再释放锁。 相信世界充满爱，即使被伤害 再回到一开始的例子，假如你往地上仍1万块钱，是不是一定会丢呢？这要看情况了，如果是在人来人往的都市，可以说肯定会丢的。如果你跑到无人区扔地上，可以说肯定不会丢。 可以看到，都是把东西无保护的放到公共区域里，结果却相差很大。这说明安全问题还和公共区域的环境状况有关系。 比如我把数据放到公共区域的堆内存中，但是始终都只会有1个线程，也就是单线程模型，那这数据肯定是安全的。 再者说，2个线程操作同一个数据和200个线程操作同一个数据，这个数据的安全概率是完全不一样的。肯定线程越多数据不安全的概率越大，线程越少数据不安全的概率越小。取个极限情况，那就是只有1个线程，那不安全概率就是0，也就是安全的。 可能你又猜到了我想表达的内容了，没错，就是CAS。可能大家觉得既然锁可以解决问题，那就用锁得了，为啥又冒出了个CAS呢？ 那是因为锁的获取和释放是要花费一定代价的，如果在线程数目特别少的时候，可能根本就不会有别的线程来操作数据，此时你还要获取锁和释放锁，可以说是一种浪费。 针对这种“地广人稀”的情况，专门提出了一种方法，叫CAS（Compare And Swap）。就是在并发很小的情况下，数据被意外修改的概率很低，但是又存在这种可能性，此时就用CAS。 假如一个线程操作数据，干了一半活，累了，想要去休息。（貌似今天的线程体质都不太好）。于是它记录下当前数据的状态（就是数据的值），回家睡觉了。 醒来后打算继续接着干活，但是又担心数据可能被修改了，于是就把睡觉前保存的数据状态拿出来和现在的数据状态比较一下，如果一样，说明自己在睡觉期间，数据没有被人动过（当然也有可能是先被改成了其它，然后又改回来了，这就是ABA问题了），那就接着继续干。如果不一样，说明数据已经被修改了，那之前做的那些操作其实都白瞎了，就干脆放弃，从头再重新开始处理一遍。 所以CAS这种方式适用于并发量不高的情况，也就是数据被意外修改的可能性较小的情况。如果并发量很高的话，你的数据一定会被修改，每次都要放弃，然后从头再来，这样反而花费的代价更大了，还不如直接加锁呢。 这里再解释下ABA问题，假如你睡觉前数据是5，醒来后数据还是5，并不能肯定数据没有被修改过。可能数据先被修改成8然后又改回到5，只是你不知道罢了。对于这个问题，其实也很好解决，再加一个版本号字段就行了，并规定只要修改数据，必须使版本号加1。 这样你睡觉前数据是5版本号是0，醒来后数据是5版本号是0，表明数据没有被修改。如果数据是5版本号是2，表明数据被改动了2次，先改为其它，然后又改回到5。 我再次相信聪明的你已经发现了，这里的CAS其实就是乐观锁，上一种方案里的获取锁和释放锁其实就是悲观锁。乐观锁持乐观态度，就是假设我的数据不会被意外修改，如果修改了，就放弃，从头再来。悲观锁持悲观态度，就是假设我的数据一定会被意外修改，那干脆直接加锁得了。 作者观点： 前两种属于隔离法，一个是位置隔离，一个是数据隔离。 然后两种是标记法，一个是只读标记，一个是加锁标记。 最后一种是大胆法，先来怼一把试试，若不行从头再来。 转自编程新说李新杰]]></content>
      <categories>
        <category>代码人生</category>
      </categories>
      <tags>
        <tag>线程安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式的7种写法]]></title>
    <url>%2F2019%2F04%2F28%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E7%9A%847%E7%A7%8D%E5%86%99%E6%B3%95%2F</url>
    <content type="text"><![CDATA[文章转自公众号无敌码农 | 谁要是再问你单例模式，那就抛给他这7种写法吧！ 单例设计模式是23种设计模式中，最基础也是最常用的设计模式之一，也是面试中关于设计模式知识点考察比较高频的问题之一。说起单例模式的写法，大多数情况下出现在我们脑海中的可能就是“饿汉式”，“懒汉式”这两种写法，但是今天小码哥今天要介绍的是单例模式的7种写法，以后面试官要是再问你单例模式，那就抛给他这七种写法吧！ 接下来，我们就言归正传，来一一介绍这七种单例模式的写法吧！ 饿汉式 饿汉式是单例模式设计中比较经典的实现方式。实现代码如下： 1234567891011121314151617//final不允许被继承public final class SingleTonEhangshi &#123; //实例变量 private byte[] data = new byte[1024]; //在定义实例对象时直接初始化 private static SingleTonEhangshi instance = new SingleTonEhangshi(); //私有化构造函数，不允许外部NEW private SingleTonEhangshi() &#123; &#125; public static SingleTonEhangshi getInstance() &#123; return instance; &#125;&#125; 饿汉式的实现关键在于instance作为类变量直接得到了初始化，如果我们主动使用SingleToEhangshi类，那么instance实例将会直接完成创建，包括其中的实例变量也都会得到初始化。 instance作为类变量，在类初始化的过程中会被收集进&lt;clinit&gt;()方法中，而该方法是可以100%地保证同步，也就是说instance在多线程的情况下不可能被初始化两次。但是由于instance被ClassLoader加载后很长一段时间才被使用的话，那就会意味着instance实例所开辟的堆内存会驻留很长的时间。 总体说来，如果一个类中的成员变量比较少，且占用的内存资源也不多，用饿汉式的方式实现单例模式也未尝不可，只是其无法进行懒加载。 懒汉式 所谓懒汉式就是在使用类实例的时候再去创建，也就是说用到的时候我再创建，这样就可以避免类在初始化的时候提前创建过早地占用内存空间。实现代码如下： 1234567891011121314151617181920//final不允许被继承public final class SingleTonLhangshi &#123; //实例变量 private byte[] data = new byte[1024]; //定义实例，但是不直接初始化 private static SingleTonLhangshi instance = null; //私有化构造函数，不允许外部NEW private SingleTonLhangshi() &#123; &#125; public static SingleTonLhangshi getInstance() &#123; if (null == instance) &#123; instance = new SingleTonLhangshi(); &#125; return instance; &#125;&#125; 类变量instance=null,因此当类被初始化的时候instance并不会立刻被实例化，而是在getInstance()方法被调用时判断instance实例是否被实例化，如果没有实例化在调用私有构造方法进行实例化操作。 懒汉式写法在多线程环境下，会存在同一时间多个线程同时看到null==instance的情况，从而导致instance会被实例化多次，从而无法保证单例的唯一性。 懒汉式＋同步方法 懒汉式的单例实现方式可以保证实例的懒加载，但是却无法保证实例的唯一性。在多线程环境下由于instance为共享数据，当多个线程访问使用时，需要保证数据的同步性，所以如果需要保证懒汉式实例的唯一性，我们可以通过同步的方式来实现。代码如下： 123456789101112131415161718192021//final不允许被继承public final class SingleTonLhangshiSync &#123; //实例变量 private byte[] data = new byte[1024]; //定义实例，但是不直接初始化 private static SingleTonLhangshiSync instance = null; //私有化构造函数，不允许外部NEW private SingleTonLhangshiSync() &#123; &#125; //向getInstance方法加入同步控制，每次只能有一个线程能够进入 public static synchronized SingleTonLhangshiSync getInstance() &#123; if (null == instance) &#123; instance = new SingleTonLhangshiSync(); &#125; return instance; &#125;&#125; 采用懒汉式＋数据同步的方法既满足了懒加载又能够100%保证instance实例的唯一性。但是，synchronized关键字的排它性会导致getInstance()方法同一时刻只能被一个线程访问，性能会比较低下。 Double-Check Double-Check是一种比较聪明的设计方式，它提供了一种高效的数据同步策略，那就是首次初始化的时候加锁，之后则允许多个线程同时进行getInstance()方法的调用来获得类的实例。代码如下： 12345678910111213141516171819202122232425262728293031//final不允许被继承public final class SingletonDoubleCheck &#123; //实例变量 private byte[] data = new byte[1024]; //定义实例，但是不直接初始化 private static SingletonDoubleCheck instance = null; Connection con; Socket socket; //私有化构造函数，不允许外部NEW private SingletonDoubleCheck(Connection con, Socket socket) &#123; this.con = con;//初始化 this.socket = socket;//初始化 &#125; public static SingletonDoubleCheck getInstance() &#123; //当instance为null时，进入同步代码块，同时该判断避免了每次都需要进入同步代码块，可以提高效率 if (null == instance) &#123; //只有一个线程能够获得SingletonDoubleCheck.class关联的monitor synchronized (SingletonDoubleCheck.class) &#123; //判断如果instance为null则创建 if (null == instance) &#123; instance = new SingletonDoubleCheck(); &#125; &#125; &#125; return instance; &#125;&#125; 当两个线程发现null==instance成立时，只有一个线程有资格进入同步代码块，完成对instance的初始化，随后的线程发现null==instance不成立则无须进行任何操作，以后对getInstance的访问就不会再需要进行数据同步了。 此种方式看起来是既满足了懒加载，又保证了instance实例的唯一性，并且还提供了比较高效的数据同步策略，可以允许多个线程同时对getInstance进行访问。但是这种方式在多线程的情况下，可能会引起空指针异常，这是因为如果在如上代码的构造方法中还存在初始化其他资源的情况的话，由于JVM运行时存在指令重排的情况，这些资源在实例化时顺序并无前后关系的约束，那么在这种情况下，就极有可能是instance最先被实例化，而con和socket并未完成实例化，而未完成实例化的实例在调用其方法时将会抛出空指针异常。 Volatile+Double-Check 为了解决Double-Check指令重排导致的空指针问题，可以用volatile关键字防止这种重排序的发生。因此代码只需要稍作修改就能满足多线程下的单例、懒加载以及实例的高效性了。代码如下： 1234567891011121314151617181920212223242526272829303132//final不允许被继承public final class SingletonDoubleCheck &#123; //实例变量 private byte[] data = new byte[1024]; //定义实例，但是不直接初始化 private static volatile SingletonDoubleCheck instance = null; Connection con; Socket socket; //私有化构造函数，不允许外部NEW private SingletonDoubleCheck(Connection con, Socket socket) &#123; this.con = con;//初始化 this.socket = socket;//初始化 &#125; public static SingletonDoubleCheck getInstance() &#123; //当instance为null时，进入同步代码块，同时该判断避免了每次都需要进入同步代码块，可以提高效率 if (null == instance) &#123; //只有一个线程能够获得SingletonDoubleCheck.class关联的monitor synchronized (SingletonDoubleCheck.class) &#123; //判断如果instance为null则创建 if (null == instance) &#123; instance = new SingletonDoubleCheck(); &#125; &#125; &#125; return instance; &#125;&#125; Holder方式 Holder方式完全借助了类加载的特点。代码如下： 12345678910111213141516171819//不允许被继承public final class SingletonHolder &#123; //实例变量 private byte[] data = new byte[1024]; private SingletonHolder() &#123; &#125; //在静态内部类中持有单例类的实例，并且可直接被初始化 private static class Holder &#123; private static SingletonHolder instance = new SingletonHolder(); &#125; //调用getInstance方法，事实上是获得Holder的instance静态属性 public static SingletonHolder getInstance() &#123; return Holder.instance; &#125;&#125; 在单例类中并没有instance的静态成员，而是将其放到了静态内部类Holder之中，因此单例类在初始化的过程中并不会创建SingletonHolder的实例，内部类Holder中定义了SingletonHolder的静态变量，并且直接进行了实例化，只有当Holder被主动引用的时候才会创建SingletonHolder的实例。 SingletonHolder实例的创建过程在Java程序编译时期收集至&lt;clinit&gt;()方法中，该方法又是同步方法，可以保证内存的可见性、JVM指令的顺序性和原子性。Holder方式的单例模式设计是最好的设计之一，也是目前使用比较广的设计。 枚举方式 枚举方式在很多开源框架中也应用得比较广泛，枚举类型不允许被继承，同样是线程安全的，并且只能被实例化一次，但是枚举类型不能够实现懒加载。用枚举类型，实现单例模式的代码如下： 1234567891011121314151617181920212223242526public class SingletonEnum &#123; //实例变量 private byte[] data = new byte[1024]; private SingletonEnum() &#123; &#125; //使用枚举充当Holder private enum EnumHolder &#123; INSTANCE; private SingletonEnum instance; EnumHolder() &#123; this.instance = new SingletonEnum(); &#125; private SingletonEnum getInstance() &#123; return instance; &#125; &#125; public static SingletonEnum getInstance() &#123; return EnumHolder.INSTANCE.getInstance(); &#125;&#125; 以上就是要给大家介绍的单例模式的7种写法了，虽然单例模式非常简单，但是在多线程的情况下，我们之前所设计的单例程序未必能够满足单实例、懒加载以及高性能的特点。]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>Java</tag>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle的一些故障处理]]></title>
    <url>%2F2019%2F04%2F08%2FOracle%E7%9A%84%E4%B8%80%E4%BA%9B%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[这里汇总了一些在使用Oracle过程中遇到的问题及解决办法，一方面做为笔记帮助自己以后更快速处理问题，一方面分享处理供大家互相学习。 ⚪SP2-0667: Message file sp1.msb not found 出错原因： crontab里面的脚本，通常读取的是默认的环境变量，PATH里面不包含oracle数据库的路径。 解决办法： 1vim ~/.bashrc 把一下内容填写其中 12export ORACLE_HOME=/u01/app/oracle/product/11.2.0/dbhome_1export PATH=$ORACLE_HOME/bin:$PATH 注意，ORACLE_HOME的路径要是你计算机中oracle真实的安装地址 环境变量设置完成，执行 source ~/.bashrc 使其生效。 ⚪ORA-12162: TNS:net service name is incorrectly specified 一般出现这种错误，基本都是环境变量配置有问题，要么是没有配置正确的ORACLE_SID、ORACLE_HOME，要么是监听配置环境变量和.bash_profile环境变量配置不一致。 这里检查发现，是操作系统环境变量没有配置ORACLE_SID 因此，我们配置一下 ~/.bashrc ，在其中添加ORACLE_SID 1export ORACLE_SID=erp 注意：ORACLE_SID的值要根据自己安装oracle时设置的为准 ⚪ORA-04021: timeout occurred while waiting to lock object 情景描述: Oracle中本来有个用户NC63PM_PEIXUN1，我把这个用户名更改为了NC63PM_PEIXUN2（更改方法请参考【Oracle更改用户名和密码】），之后我想按照旧的用户名再创建一个用户，但是创建的用户的SQL语句执行了十五分钟还没执行完，并报如下的错误： ORA-04021: timeout occurred while waiting to lock object 解决办法 查看是否被锁表了 123&gt; SELECT object_name,machine,s.sid,s.serial#&gt; FROM v$locked_object l,dba_objects o ,v$session s&gt; WHERE l.object_id=o.object_id AND l.session_id=s.sid; 发现没有被锁表 查看锁表 使用 DBA_DDL_LOCKS视图获得DDL锁定信息 1&gt; SELECT * FROM dba_ddl_locks; 发现有两条关于 NC63PM_PEIXUN1 用户的锁定信息 通过 session_id 找到对应的锁表信息 1&gt; SELECT sid,serial#,status FROM v$session a WHERE a.sid in (829,392); 注：因我是kill掉这两条信息后才截的图，所以 STATUS 才为 KILLED 的。 kill这两条锁表 12&gt; ALTER SYSTEM KILL SESSION &apos;392, 5049&apos;;&gt; ALTER SYSTEM KILL SESSION &apos;829, 25287&apos;; 再次执行创建用户的脚本就能顺利执行。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础知识小结]]></title>
    <url>%2F2019%2F03%2F26%2FJava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[写这篇文章是为了记录一下学习中被忽略的知识点，这些知识点虽然知道听说过，但对它们的概念和作用都很模糊，如果别人问起为什么，自己还真解释不上来，因此做个记录，方便以后回顾以及大家一起学习。 一个类的构造方法的作用是什么 若一个类没有声明构造方法,该程序能正确执行吗 ?为什么? 主要作用是完成对类对象的初始化工作。可以执行。因为一个类即使没有声明构造方法也会默认生成一个不带参数且没有任何执行动作的构造方法。 在Java中定义一个不做任何事且没有参数的构造方法的作用 Java程序在执行子类的构造方法之前，如果没有用super()来调用父类特定的构造方法，则系统会自动调用父类中没有参数的构造方法。当此时父类中只定义了有参数的构造方法而没有定义无参数的构造方法，然后在子类的构造方法中又没有使用super()来调用父类中特定的构造方法，则编译时将发生错误，因为Java程序在父类中找不到无参数的构造方法可供执行。所以我们要在父类里加上一个不做任何事且没有参数的构造方法。 重载和重写的区别 重载：发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。 重写：发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类；如果父类方法访问修饰符为private则子类就不能重写该方法。 try-catch-finally try块：用于捕获异常。其后可跟零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch块： 用于处理try捕获的异常。 finally块：无论是否捕获或者处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。 注意，一下几种情况finally块不会被执行： 1.在finally语句块中发生了异常； 2.在前面的代码中用了System.exit()退出； 3.程序所有的线程死亡； 4.关闭CPU。 final/finally/finalize的区别 final Java中，final关键字可以用来修饰类、方法和变量（包括成员变量和局部变量） 当用final修饰一个类时，表明这个类不能被继承 当final修饰一个类中的某个方法，这个类的子类不能重写覆盖这个被修饰的类，也就是说子类是不能够存在和父类一模一样的方法。 final修饰变量，该变量表示常量，只能被赋值一次，赋值后值不能被修改。 finally 在异常处理时提供finally块来执行清楚操作。论是否捕获或者处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。 finalize 是方法名。java技术允许使用finalize()方法在垃圾收集器将对象从内存中清除之前做必要的清理工作。这个方法是在垃圾收集器在确定了，被清理对象没有被引用的情况下调用的。 finalize是在Object类中定义的，因此，所有的类都继承了它。子类可以覆盖finalize()方法，来整理系统资源或者执行其他清理工作。 HashMap HashMap的特点 HashMap是基于哈希表的Map接口实现的。 HashMap底层采用的是Entry数组和链表实现的。 HashMap是采用key-value形式存储，其中key是可以允许为null，但是只能有一个，并且key不允许重复（如果重复则新值会覆盖旧值）。 HashMap是线程不安全的。 HashMap存入的顺序和遍历的顺序可能不一致（无序）。 HashMap保存数据的时候通过计算key的hash值来决定存储的位置。 HashMap的工作原理是什么？ HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，计算并返回的hashCode是用于找到Map数组的bucket位置来储存Node 对象。这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Node 。 HashMap源代码 要看HashMap的源代码，我们还是从HashMap的构造方法开始一步一步的讲解。 小总结：可以看出HashMap构造的时候会初始化16个容量，并且负载因子是0.75。负载因子是什么呢？我们后面讲。 小总结：这个构造方法没有什么可说的，只是多了些验证。在这里呢？构造方法就算初始化完毕了。 我们知道HashMap最常用的方法也就是put方法了，那么下面我们就着重去探究一下put方法的实现原理，也就是对HashMap的一个透彻理解。 put方法注释说明 这段代码好好的研读，请仔细往下看： 第一步： 直接判断 table==EMPTY_TABLE ，那么这个table是什么呢？看下图： 那么这个Entry又是说什么东东呢？ 这个Entry是Map的一个静态内部类，里面最重要的属性有key、value和next三个属性值，在这里，我想大家已经猜到了，这个key和value是不是我们put的时候的key和value呢？答案是的，这个next又是干嘛用的呢？实际上这个Entry的的数据结构是一个单链表，这个next的属性的值还是这个Entry，表示的是当前的节点的下一个节点是哪个Entry。 好啦，源代码看到这里，我们知道，在put方法中，直接判断table是否为null，那么很显然到目前为止我们的table肯定是为null的，那么继续看如果table为null则要执行的代码。看下图： 哇塞，可以很直观的看到，我们实际上是初始化了一个Entry数组，而我们HashMap中的数据都是保存在了Entry[]里面了。 小总结：HashMap其实就是一个线性的Entry数组，而在Entry这个对象中保存了key和value，至于Entry对象中的next的具体作用是干嘛的，稍等做介绍哦。 第二步： 判断key是否为null。从这里可以看出，当判断key如果为null的话，并没有抛出什么异常错误，很显然HashMap是支持key为null的。那么就来看看key如果为null，会怎么处理呢？ 小总结：首先去循环遍历这个Entry数组，判断是否有key为null的情况，如果有则新值覆盖掉旧值。如果没有key为null的情况，则hash值为0，数据存储在这个Entry数组的第0个位置，也就是table[0]，具体方法可以查看addEntry方法，在这里呢，我就不再演示了。 第三步： 通过hash方法对key进行计算hash散列值，并且根据这个散列值查找这个要保存的值应该存储到table这个数组中的哪个索引位置。 第四步： 循环变量这个Entry数组，并且判断是否有重复的元素添加进去。 小总结：当去变量这个Entry数组的时候，去判断两个Entry对象的key的hash是否相同则仅仅表示它们的存储位置是相同的，然后继续判断两个Entry的key是否相等或者equals是否相等，如果条件都满足，则表示要添加的元素，key已经重复，则直接将新值覆盖掉旧值，并且return返回，一旦条件不满足，则直接将添加的元素添加到Entry对象中。 好啦，这个就是整个HashMap的底层原理。现在有的朋友可能会有产生这样的问题：如果计算的key的hash值相等，但是equals方法不相等，那么计算出来的要存储的位置不就冲突了吗？那么如果保存呢？ 解决：实际上这种担忧是有必要的，因为我们完全有可能就是说计算的key的hash值和另一个key的hash值是相等的，那么这个时候呢，如果key的equals方法又不相等，那么这个时候我要保存的value值应该存储到table中的哪个索引上呢？实际上，这种情况叫做hash冲突，学习过数据结构的朋友应该都知道，解决hash冲突的方法有很多，但是在Java中，解决冲突的办法是采用的是链表来解决的。还记得这个Entry的next属性吗？对了，这个next属性就是用来记录这个链表上的下一个Entry。 HashMap的内容摘自http://baijiahao.baidu.com/s?id=1601416041995350500&amp;wfr=spider&amp;for=pc volatile关键字的基本作用和原理 volatile关键字可以实现线程间的可见性，之所以可以实现这一点，原因在于JVM会保证被volatile修饰的变量，在线程栈中被线程使用时都会主动从共享内存(堆内存/主内存)中以实时的方式同步一次；另一方面，如果线程在工作内存中修改了volatile修饰的变量，也会被JVM要求立马刷新到共享内存中去。因此，即便某个线程修改了该变量，其他线程也可以立马感知到变化从而实现可见性. 未完待续...]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle存储过程的实现]]></title>
    <url>%2F2018%2F03%2F03%2FOracle%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Oracle存储过程是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中。用户通过指定存储过程的名字并给出参数（是否给参数要看该存储过程定义的过程中是否设置了参数）来执行它。 准备工作 创建一张测试表 students 12345678910create table STUDENTS( id VARCHAR2(50) default sys_guid() not null, name VARCHAR2(20), age NUMBER(4,1), school VARCHAR2(100), grade VARCHAR2(50), address VARCHAR2(500), remarks VARCHAR2(500), ts CHAR(19) default to_char(sysdate,'yyyy-mm-dd hh24:mi:ss')) 插入一条测试数据。当然，也可以插入自己想插入的内容。 12insert into STUDENTS (id, name, age, school, grade, address, remarks, ts)values ('8A17DE17428E45D6E0530100007FABEB', 'xiaoming', 20, 'Changchun University of Architecture', 'Junior', 'Changchun, Jilin', null, '2019-06-12 11:07:57'); 第一个简单的存储过程 123456CREATE OR REPLACE PROCEDURE stu_schoolAS school_name VARCHAR2(100);BEGIN SELECT school INTO school_name FROM students WHERE ID='8A17DE17428E45D6E0530100007FABEB'; dbms_output.put_line(school_name);END; 执行存储过程，可以在PLSQL对象中看到我们刚才新创建的存储过程，并且没有报错，代表编译成功。 简单的存储过程 执行存储过程 1CALL stu_school(); 调用时，"()"是必不可少的，无论是有参数还是无参数 在SQL窗口输出页签中可以看到正确的输出内容 输出 四种存储过程 存储过程有一下四种情况 - 无参数存储过程 - 仅有输入参数存储过程 - 仅有输出参数存储过程 - 既有输入又有输出存储过程 下面将对这四种存储过程分别举例说明 无参数存储过程 无参数存储过程就如上面写的那个简单的存储过程，也可以这样写： 123456CREATE OR REPLACE PROCEDURE stu_schoolAS school_name students.school%TYPE;BEGIN SELECT school INTO school_name FROM students WHERE ID='8A17DE17428E45D6E0530100007FABEB'; dbms_output.put_line(school_name);END; 仅有输入参数存储过程 123456CREATE OR REPLACE PROCEDURE stu_address(stu_id IN students.id%TYPE)AS addr students.address%TYPE;BEGIN SELECT address INTO addr FROM students WHERE ID=stu_id; dbms_output.put_line(addr);END; 执行存储过程 1CALL stu_address('8A17DE17428E45D6E0530100007FABEB'); 仅有输入参数存储过程 仅有输出参数存储过程 12345CREATE OR REPLACE PROCEDURE stu_age(stu_age OUT students.age%TYPE)ASBEGIN SELECT age INTO stu_age FROM students WHERE ID='8A17DE17428E45D6E0530100007FABEB';END; 需要注意的是，此种存储过程不能直接通过call来调用，需要通过一下方式执行 注意，如果通过这种方式执行存储过程，要记得在存储过程中添加输出语句，不然的话，纵然执行成功，也没有结果输出。 dbms_output.put_line(stu_age); 12345DECLAREstuage students.age%TYPE;BEGIN stu_age(stuage);END; 或者通过oracle函数调用带有输出参数的存储过程 12345CREATE OR REPLACE FUNCTION get_stuage(stuage OUT NUMBER) RETURN NUMBER ISBEGIN stu_age(stuage); RETURN stuage;END; 执行函数 12345DECLARE stuage students.age%TYPE;BEGIN dbms_output.put_line('return result:' || get_stuage(stuage));END; 既有输入又有输出参数的存储过程 12345CREATE OR REPLACE PROCEDURE stu_name(stuid IN students.id%TYPE, stuname OUT students.name%TYPE)ASBEGIN SELECT NAME INTO stuname FROM students WHERE ID=stuid;END; 新建存储函数调用存储过程 12345CREATE OR REPLACE FUNCTION get_stuname(stuid IN students.id%TYPE, stuname OUT students.name%TYPE) RETURN VARCHAR2 ISBEGIN stu_name(stuid, stuname); RETURN stuname;END; 执行函数 12345DECLARE stuname students.name%TYPE;BEGIN dbms_output.put_line('The student name is:' || get_stuname('8A17DE17428E45D6E0530100007FABEB', stuname));END; Java调用存储过程 Java调用仅有输出参数的存储过程 针对存储过程 stu_age 12345CREATE OR REPLACE PROCEDURE stu_age(stu_age OUT students.age%TYPE)ASBEGIN SELECT age INTO stu_age FROM students WHERE ID='8A17DE17428E45D6E0530100007FABEB';END; Java代码如下： 123456789101112131415private void OnlyOutputProcedure() &#123; try &#123; Class.forName(DRVIER); Connection connection = DriverManager.getConnection(URL, USERNAME, PASSWORD); String sql = "&#123;call stu_age(?)&#125;"; CallableStatement statement = connection.prepareCall(sql); statement.registerOutParameter(1, OracleTypes.NUMBER); statement.execute(); System.out.println(statement.getInt(1)); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;&#125; Java调用既有输入参数又有输出参数的存储过程 针对存储过程 stu_name 12345CREATE OR REPLACE PROCEDURE stu_name(stuid IN students.id%TYPE, stuname OUT students.name%TYPE)ASBEGIN SELECT NAME INTO stuname FROM students WHERE ID=stuid;END; Java代码如下： 12345678910111213141516private void InAndOutputProcedure() &#123; try &#123; Class.forName(DRVIER); Connection connection = DriverManager.getConnection(URL, USERNAME, PASSWORD); String sql = "&#123;call stu_name(?,?)&#125;"; CallableStatement statement = connection.prepareCall(sql); statement.setString(1, "8A17DE17428E45D6E0530100007FABEB"); statement.registerOutParameter(2, OracleTypes.VARCHAR); statement.execute(); System.out.println(statement.getString(2)); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;&#125; 以上是有OUT输出参数的存储过程，Java在调用存储过程后还会获得存储过程返回的参数。那么如果存储过程没有OUT输出参数怎么办？ Java调用仅有输入参数的存储过程 1234567891011121314private void OnlyInputProcedure() &#123; try &#123; Class.forName(DRVIER); Connection connection = DriverManager.getConnection(URL, USERNAME, PASSWORD); String sql = "&#123;call stu_address(?)&#125;"; CallableStatement statement = connection.prepareCall(sql); statement.setString(1, "8A17DE17428E45D6E0530100007FABEB"); statement.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;&#125; Java调用无参的存储过程 12345678910111213private void NoParameterProcedure() &#123; try &#123; Class.forName(DRVIER); Connection connection = DriverManager.getConnection(URL, USERNAME, PASSWORD); String sql = "&#123;call stu_school()&#125;"; CallableStatement statement = connection.prepareCall(sql); statement.execute(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;&#125; Java调用存储函数 创建一个存储函数 get_address 调用 123456create or replace function get_address(stuid in students.id%type) return varchar2 isstuaddress students.address%type;begin select address into stuaddress from students where id=stuid; return stuaddress;end; Java调用存储过程： 12345678910111213141516private void NoParameterProcedure() &#123; try &#123; Class.forName(DRVIER); Connection connection = DriverManager.getConnection(URL, USERNAME, PASSWORD); String sql = "&#123;?=call get_address(?)&#125;"; CallableStatement statement = connection.prepareCall(sql); statement.registerOutParameter(1, Types.VARCHAR); statement.setString(2, "8A17DE17428E45D6E0530100007FABEB"); statement.execute(); System.out.println(statement.getString(1)); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;&#125; 这里有一个关于Oracle存储过程的PPT文档，供大家下载学习点击下载]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
        <tag>存储过程</tag>
      </tags>
  </entry>
</search>
